<!DOCTYPE html> 
<html lang="en" xml:lang="en" > 
<head> <title>Analysis Theorems</title> 
<meta  charset="UTF-8"" /> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" /> 
<link rel="stylesheet" type="text/css" href="Analysis.css" /> 
<meta name="src" content="Analysis.tex" /> 
<script>window.MathJax = { tex: { tags: "ams", inlineMath: [ ["\\\(","\\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true, processEnvironments: true, packages: ['base', 'color', 'ams', 'newcommand'] }, loader: { load: ['[tex]/color', '[tex]/ams', '[tex]/newcommand'] } }; </script> 
 <script type="text/javascript" async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>  
</head><body 
>
  <p style="display: none;">\( \newcommand{\cO}{\mathcal{O}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\SP}{\mathbb{S}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\FF}{\mathbb{F}}
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\ts}{\textsuperscript}
\newcommand{\mf}{\mathfrak}
\newcommand{\cc}{\mf{c}}
\newcommand{\mg}{\mf{g}}
\newcommand{\ma}{\mf{a}}
\newcommand{\mh}{\mf{h}}
\newcommand{\mn}{\mf{n}}
\newcommand{\mc}{\mf{c}}
\newcommand{\ul}{\underline}
\newcommand{\mz}{\mf{z}}
\newcommand{\me}{\mf{e}}
\newcommand{\mff}{\mf{f}}
\newcommand{\mm}{\mf{m}}
\newcommand{\mt}{\mf{t}}
\newcommand{\pp}{\mf{p}}
\newcommand{\qq}{\mf{q}}
\newcommand{\gl}{\mf{gl}}
\newcommand{\msl}{\mf{sl}}
\newcommand{\so}{\mf{so}}
\newcommand{\mfu}{\mf{u}}
\newcommand{\su}{\mf{su}}
\newcommand{\msp}{\mf{sp}}
\renewcommand{\aa}{\mf{a}}
\newcommand{\bb}{\mf{b}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\lb}{\langle}
\newcommand{\rb}{\rangle}
\newcommand{\ff}{\mf{f}}
\newcommand{\ee}{\epsilon}
\newcommand{\heart}{\heartsuit}

\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

\newcommand{\pushout}{\arrow[ul, phantom, "\ulcorner", very near start]}
\newcommand{\pullback}{\arrow[dr, phantom, "\lrcorner", very near start]}

\newcommand{\simp}[1]{#1^{\Delta^{op}}}

\newcommand{\arrowtcupp}[2]{\arrow[bend left=50, ""{name=U, below,inner sep=1}]{#1}\arrow[Rightarrow,from=U,to=MU,"#2"]}
\newcommand{\arrowtclow}[2]{\arrow[bend right=50, ""{name=L,inner sep=1}]{#1}\arrow[Rightarrow,from=LM,to=L]{}[]{#2}} % if you want to change some parameter of the label.
\newcommand{\arrowtcmid}[2]{\arrow[""{name=MU,inner sep=1},""{name=LM,below,inner sep=1}]{#1}[pos=.1]{#2}}
\newcommand{\dummy}{\textcolor{white}{\bullet}}


%for adjunction
\newcommand{\adjunction}[4]{
	#1\hspace{2pt}\colon #2	\leftrightharpoons #3 \hspace{2pt}\colon #4
}


%Math operators

\newcommand{\aug}{\mathop{\rm aug}\nolimits}
\newcommand{\MC}{\mathop{\rm MC}\nolimits}
\newcommand{\art}{\mathop{\rm art}\nolimits}
\newcommand{\DiGrph}{\mathop{\rm DiGrph}\nolimits}
\newcommand{\FMP}{\mathop{\rm FMP}\nolimits}
\newcommand{\CAlg}{\mathop{\rm CAlg}\nolimits}
\newcommand{\perf}{\mathop{\rm perf}\nolimits}
\newcommand{\cof}{\mathop{\rm cof}\nolimits}
\newcommand{\fib}{\mathop{\rm fib}\nolimits}
\newcommand{\Thick}{\mathop{\rm Thick}\nolimits}
\newcommand{\Orb}{\mathop{\rm Orb}\nolimits}
\newcommand{\ko}{\mathop{\rm ko}\nolimits}
\newcommand{\Spf}{\mathop{\rm Spf}\nolimits}
\newcommand{\Spc}{\mathop{\rm Spc}\nolimits}
\newcommand{\sk}{\mathop{\rm sk}\nolimits}
\newcommand{\cosk}{\mathop{\rm cosk}\nolimits}
\newcommand{\holim}{\mathop{\rm holim}\nolimits}
\newcommand{\hocolim}{\mathop{\rm hocolim}\nolimits}
\newcommand{\Pre}{\mathop{\rm Pre}\nolimits}
\newcommand{\THR}{\mathop{\rm THR}\nolimits}
\newcommand{\THH}{\mathop{\rm THH}\nolimits}
\newcommand{\Fun}{\mathop{\rm Fun}\nolimits}
\newcommand{\Loc}{\mathop{\rm Loc}\nolimits}
\newcommand{\Bord}{\mathop{\rm Bord}\nolimits}
\newcommand{\Cob}{\mathop{\rm Cob}\nolimits}
\newcommand{\Set}{\mathop{\rm Set}\nolimits}
\newcommand{\Ind}{\mathop{\rm Ind}\nolimits}
\newcommand{\Sind}{\mathop{\rm Sind}\nolimits}
\newcommand{\Ext}{\mathop{\rm Ext}\nolimits}
\newcommand{\sd}{\mathop{\rm sd}\nolimits}
\newcommand{\Ex}{\mathop{\rm Ex}\nolimits}
\newcommand{\Out}{\mathop{\rm Out}\nolimits}
\newcommand{\Cyl}{\mathop{\rm Cyl}\nolimits}
\newcommand{\Path}{\mathop{\rm Path}\nolimits}
\newcommand{\Ch}{\mathop{\rm Ch}\nolimits}
\newcommand{\SSet}{\mathop{\rm \Set^{\Delta^{op}}}\nolimits}
\newcommand{\Sq}{\mathop{\rm Sq}\nolimits}
\newcommand{\Free}{\mathop{\rm Free}\nolimits}
\newcommand{\Map}{\mathop{\rm Map}\nolimits}
\newcommand{\Chain}{\mathop{\rm Ch}\nolimits}
\newcommand{\LMap}{\mathop{\rm LMap}\nolimits}
\newcommand{\RMap}{\mathop{\rm RMap}\nolimits}
\newcommand{\Tot}{\mathop{\rm Tot}\nolimits}
\newcommand{\MU}{\mathop{\rm MU}\nolimits}
\newcommand{\MSU}{\mathop{\rm MSU}\nolimits}
\newcommand{\MSp}{\mathop{\rm MSp}\nolimits}
\newcommand{\MSO}{\mathop{\rm MSO}\nolimits}
\newcommand{\MO}{\mathop{\rm MO}\nolimits}
\newcommand{\BU}{\mathop{\rm BU}\nolimits}
\newcommand{\BSU}{\mathop{\rm BSU}\nolimits}
\newcommand{\BSp}{\mathop{\rm BSp}\nolimits}
\newcommand{\BGL}{\mathop{\rm BGL}\nolimits}
\newcommand{\BSO}{\mathop{\rm BSO}\nolimits}
\newcommand{\BO}{\mathop{\rm BO}\nolimits}
\newcommand{\Tor}{\mathop{\rm Tor}\nolimits}
\newcommand{\Cotor}{\mathop{\rm Cotor}\nolimits}
\newcommand{\imag}{\mathop{\rm Im}\nolimits}
\newcommand{\real}{\mathop{\rm Re}\nolimits}
\newcommand{\Cat}{\mathop{\rm Cat}\nolimits}
\newcommand{\Fld}{\mathop{\rm Fld}\nolimits}
\newcommand{\Frac}{\mathop{\rm Frac}\nolimits}
\newcommand{\Dom}{\mathop{\rm Dom}\nolimits}
\newcommand{\Hotc}{\mathop{\rm Hotc}\nolimits}
\newcommand{\Top}{\mathop{\rm Top}\nolimits}
\newcommand{\Ring}{\mathop{\rm Ring}\nolimits}
\newcommand{\CRing}{\mathop{\rm CRing}\nolimits}
\newcommand{\CGHaus}{\mathop{\rm CGHaus}\nolimits}
\newcommand{\Alg}{\mathop{\rm Alg}\nolimits}
\newcommand{\Bool}{\mathop{\rm Bool}\nolimits}
\newcommand{\hTop}{\mathop{\rm hTop}\nolimits}
\newcommand{\Nat}{\mathop{\rm Nat}\nolimits}
\newcommand{\Rel}{\mathop{\rm Rel}\nolimits}
\newcommand{\Mod}{\mathop{\rm Mod}\nolimits}
\newcommand{\Space}{\mathop{\rm Space}\nolimits}
\newcommand{\Vect}{\mathop{\rm Vect}\nolimits}
\newcommand{\FinVect}{\mathop{\rm FinVect}\nolimits}
\newcommand{\Matr}{\mathop{\rm Matr}\nolimits}
\newcommand{\Ab}{\mathop{\rm Ab}\nolimits}
\newcommand{\Gr}{\mathop{\rm Gr}\nolimits}
\newcommand{\Grp}{\mathop{\rm Grp}\nolimits}
\newcommand{\Hol}{\mathop{\rm Hol}\nolimits}
\newcommand{\Gpd}{\mathop{\rm Gpd}\nolimits}
\newcommand{\Grpd}{\mathop{\rm Gpd}\nolimits}
\newcommand{\Mon}{\mathop{\rm Mon}\nolimits}
\newcommand{\FinSet}{\mathop{\rm FinSet}\nolimits}
\newcommand{\Sch}{\mathop{\rm Sch}\nolimits}
\newcommand{\AffSch}{\mathop{\rm AffSch}\nolimits}
\newcommand{\Idem}{\mathop{\rm Idem}\nolimits}
\newcommand{\SIdem}{\mathop{\rm SIdem}\nolimits}
\newcommand{\Aut}{\mathop{\rm Aut}\nolimits}
\newcommand{\Ord}{\mathop{\rm Ord}\nolimits}
\newcommand{\coker}{\mathop{\rm coker}\nolimits}
\newcommand{\ch}{\mathop{\rm char}\nolimits}%characteristic
\newcommand{\Sym}{\mathop{\rm Sym}\nolimits}
\newcommand{\adj}{\mathop{\rm adj}\nolimits}
\newcommand{\dil}{\mathop{\rm dil}\nolimits}
\newcommand{\Cl}{\mathop{\rm Cl}\nolimits}
\newcommand{\Diff}{\mathop{\rm Diff}\nolimits}
\newcommand{\End}{\mathop{\rm End}\nolimits}
\newcommand{\Hom}{\mathop{\rm Hom}\nolimits}% preferred
\newcommand{\Gal}{\mathop{\rm Gal}\nolimits}
\newcommand{\Pos}{\mathop{\rm Pos}\nolimits}
\newcommand{\Ad}{\mathop{\rm Ad}\nolimits}
\newcommand{\GL}{\mathop{\rm GL}\nolimits}
\newcommand{\SL}{\mathop{\rm SL}\nolimits}
\newcommand{\vol}{\mathop{\rm vol}\nolimits}
\newcommand{\reg}{\mathop{\rm reg}\nolimits}
\newcommand{\Or}{\text{O}}
\newcommand{\U}{\mathop{\rm U}\nolimits}
\newcommand{\SOr}{\mathop{\rm SO}\nolimits}
\newcommand{\SU}{\mathop{\rm SU}\nolimits}
\newcommand{\Spin}{\mathop{\rm Spin}\nolimits}
\newcommand{\Sp}{\mathop{\rm Sp}\nolimits}
\newcommand{\Int}{\mathop{\rm Int}\nolimits}
\newcommand{\im}{\mathop{\rm im}\nolimits}
\newcommand{\dom}{\mathop{\rm dom}\nolimits}
\newcommand{\di}{\mathop{\rm div}\nolimits}
\newcommand{\cod}{\mathop{\rm cod}\nolimits}
\newcommand{\colim}{\mathop{\rm colim}\nolimits}
\newcommand{\ad}{\mathop{\rm ad}\nolimits}
\newcommand{\PSL}{\mathop{\rm PSL}\nolimits}
\newcommand{\PGL}{\mathop{\rm PGL}\nolimits}
\newcommand{\sep}{\mathop{\rm sep}\nolimits}
\newcommand{\MCG}{\mathop{\rm MCG}\nolimits}
\newcommand{\oMCG}{\mathop{\rm MCG^+}\nolimits}
\newcommand{\Spec}{\mathop{\rm Spec}\nolimits}
\newcommand{\rank}{\mathop{\rm rank}\nolimits}
\newcommand{\diverg}{\mathop{\rm div}\nolimits}%Divergence
\newcommand{\disc}{\mathop{\rm disc}\nolimits}
\newcommand{\sign}{\mathop{\rm sign}\nolimits}
\newcommand{\Arf}{\mathop{\rm Arf}\nolimits}
\newcommand{\Pic}{\mathop{\rm Pic}\nolimits}
\newcommand{\Tr}{\mathop{\rm Tr}\nolimits}
\newcommand{\res}{\mathop{\rm res}\nolimits}
\newcommand{\Proj}{\mathop{\rm Proj}\nolimits}
\newcommand{\mult}{\mathop{\rm mult}\nolimits}
\newcommand{\N}{\mathop{\rm N}\nolimits}
\newcommand{\lk}{\mathop{\rm lk}\nolimits}
\newcommand{\Pf}{\mathop{\rm Pf}\nolimits}
\newcommand{\sgn}{\mathop{\rm sgn}\nolimits}
\newcommand{\grad}{\mathop{\rm grad}\nolimits}
\newcommand{\lcm}{\mathop{\rm lcm}\nolimits}
\newcommand{\Ric}{\mathop{\rm Ric}\nolimits}
\newcommand{\Hess}{\mathop{\rm Hess}\nolimits}
\newcommand{\sn}{\mathop{\rm sn}\nolimits}
\newcommand{\cut}{\mathop{\rm cut}\nolimits}
\newcommand{\tr}{\mathop{\rm tr}\nolimits}
\newcommand{\codim}{\mathop{\rm codim}\nolimits}
\newcommand{\ind}{\mathop{\rm index}\nolimits}
\newcommand{\rad}{\mathop{\rm rad}\nolimits}
\newcommand{\Rep}{\mathop{\rm Rep}\nolimits}
\newcommand{\Lie}{\mathop{\rm Lie}\nolimits}
\newcommand{\Der}{\mathop{\rm Der}\nolimits}
\newcommand{\hgt}{\mathop{\rm ht}\nolimits}
\newcommand{\Ider}{\mathop{\rm Ider}\nolimits}
\newcommand{\id}{\mathop{\rm id}\nolimits} \)</p>
  <div class="maketitle">
  <h2 class="titleHead">
ANALYSIS THEOREMS
  </h2>
  <div class="authors"><span class="author" >
<span 
class="cmr-10">ISHAN LEVY</span>
  </span></div>
<div class="submaketitle">
</div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">1. </span> <a 
 id="x1-10001"></a>Set Theory</h3>
<!--l. 10--><p class="noindent" >Here are some basic theorems from introductory analysis.
</p>
  <div class="newtheorem">
<!--l. 12--><p class="noindent" ><span class="head">
<a 
 id="x1-1001r1"></a>
<span 
class="cmbx-12">Theorem 1.1 </span>&#x0028;Cantor-Bernstein&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(f:X \to Y\) <span 
class="cmti-12">and</span> \(g:Y \to X\) <span 
class="cmti-12">are injections,</span> \(|X|=|Y|\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 16--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We will construct a function \(h: X \to Y\) that is a bijection. First, de&#xFB01;ne \(C \subset X\) as \(\bigcup _{n \in \NN \cup 0}(g \circ f)^n(A-g(B))\). Then we de&#xFB01;ne \[ h(x) = \begin{cases} f &amp; x \in C \\ g^{-1} &amp; x \notin C \end{cases} \]
We can now check this is a bijection. For injectivity, suppose \(h(a)=h(b)\), we&#x2019;d like to show \(a = b\). As \(f,g^{-1}\) are
injective, it su&#xFB03;ces to suppose \(a \in C, b \notin C\) and &#xFB01;nd a contradiction. Then \(g^{-1}(b)=h(b)=h(a)=f(a)=f\circ (g\circ f)^n(c)\), but by applying \(g\) to each
side of this equation we have \(b \in C\), a contradiction.
</p><!--l. 25--><p class="indent" >  For surjectivity, let \(y \in Y\). Suppose \(g(y) \notin C\). Then \(h(g(y)) = y\). Now suppose \(g(y) \in C\). Then \(g(y) = (g\circ f)^n(c)\) with \(c \notin g(B)\), which implies \(n\geq 1\). Then
by injectivity of \(g\) we get \(y = f((g\circ f)^{n-1}(c)) = h((g\circ f)^{n-1}(c))\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
                                                                                         
                                                                                         
  <h3 class="sectionHead"><span class="titlemark">2. </span> <a 
 id="x1-20002"></a>Inequalities</h3>
  <div class="newtheorem">
<!--l. 31--><p class="noindent" ><span class="head">
<a 
 id="x1-2001r1"></a>
<span 
class="cmbx-12">Theorem 2.1 </span>&#x0028;Cauchy-Schwarz Inequality&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">In an inner product space,</span> \(|(x,y)| \leq \Vert x \Vert \Vert y \Vert \) <span 
class="cmti-12">with equality holding i&#xFB00;</span> \(x\)
<span 
class="cmti-12">and</span> \(y\) <span 
class="cmti-12">are linearly dependent.</span>
</p>
  <div class="proof">
<!--l. 34--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>After multiplying \(x\) by an element of \(S^1\), we may assume \((x,y)\) is real. Consider \(z = y - \frac{(x,y)}{\Vert x\Vert ^2}x\), the projection
of \(y\) onto the orthogonal complement of \(x\). Indeed \(x,z\) are orthogonal as \((z,x) = (y,x) - \frac{(x,y)}{\Vert x\Vert ^2}(x,x) = 0\). Then we have: \[ 0 \leq (z,z) = (z,y) - \frac{(x,y)}{\Vert x\Vert ^2}(z,x) = (z,y) = (y,y) - \frac{(x,y)}{\Vert x \Vert ^2}(x,y) \] which after
rearrangement is what we want. Note that the equality above happens i&#xFB00; \(z = 0\) which happens i&#xFB00;
\(x\) and \(y\) are linearly dependent. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 43--><p class="noindent" ><span class="head">
<a 
 id="x1-2002r2"></a>
<span 
class="cmbx-12">Theorem 2.2 </span>&#x0028;AM-GM Inequality&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">Let</span> \(x_1, \dots ,x_n\) <span 
class="cmti-12">be non-negative reals. Then</span> \(\prod _1^nx_i\leq \big (\frac{\sum _1^nx_i}{n}\big )^n\) <span 
class="cmti-12">with equality holding i&#xFB00;</span>
\(x_1 = \dots = x_n\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 46--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We suppose some of the \(x_i\) are not equal and show the strict inequality via induction.
If \(\mu \) denotes \(\frac{\sum _1^nx_i}{n}\) then WLOG we may assume \(x_n&gt;\mu &gt;x_{n-1}\). Then de&#xFB01;ne \(y = x_n+x_{n-1}-\mu \) and note \(y\) is non-negative. Then by
induction, we have \[ y\prod _1^{n-2}x_i\leq \big (\frac{y+\sum _1^{n-2}x_i}{n-1}\big )^{n-1} = \mu ^{n-1} \] Multiplying by \(\mu \), we get \[ \mu y\prod _1^{n-2}x_i\leq \mu ^{n} \] so it su&#xFB03;ces to show \(\mu y&gt;x_nx_{n-1}\), but this is true as \[ y\mu -x_nx_{n-1} = (x_n+x_{n-1}-\mu )\mu -x_nx_{n-1} = (x_n-\mu )(\mu -x_{n-1})&gt;0 \] <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 61--><p class="noindent" >Jensen&#x2019;s Inequality is a powerful inequality. To prove it in its measure-theoretic form, we need the
notion of a subderivative and a convex function.
</p>
  <div class="newtheorem">
                                                                                         
                                                                                         
<!--l. 63--><p class="noindent" ><span class="head">
<a 
 id="x1-2003r3"></a>
<span 
class="cmbx-12">De&#xFB01;nition 2.3.</span>  </span><span 
class="cmti-12">Let</span> \(A \subset V\) <span 
class="cmti-12">be a convex subset of a real vector space. A function</span> \(f: A \to \RR \) <span 
class="cmti-12">is convex if</span> \(f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)\) <span 
class="cmti-12">for</span>
\(t \in [0,1]\)<span 
class="cmti-12">. It is </span><span 
class="cmbxti-10x-x-120">strictly convex</span><span 
class="cmti-12">, if the inequality is strict for</span> \(x\neq y\)<span 
class="cmti-12">,</span>\(t\neq 0,1\)<span 
class="cmti-12">.</span>
</p>
  </div>
<!--l. 67--><p class="indent" >  The following lemma is obvious.
</p>
  <div class="newtheorem">
<!--l. 69--><p class="noindent" ><span class="head">
<a 
 id="x1-2004r4"></a>
<span 
class="cmbx-12">Lemma 2.4.</span>  </span>\(f\) <span 
class="cmti-12">is convex i&#xFB00; its graph with the set of points lying above it is convex.</span>
</p>
  </div>
  <div class="newtheorem">
<!--l. 73--><p class="noindent" ><span class="head">
<a 
 id="x1-2005r5"></a>
<span 
class="cmbx-12">De&#xFB01;nition 2.5.</span>  </span><span 
class="cmti-12">Let</span> \(f:\RR \to \RR \) <span 
class="cmti-12">be convex. A </span><span 
class="cmbxti-10x-x-120">subderivative </span><span 
class="cmti-12">of</span> \(f\) <span 
class="cmti-12">at</span> \(p\) <span 
class="cmti-12">is a</span> \(c\) <span 
class="cmti-12">such that</span> \(f(x)-f(p) \geq c(x-p)\)<span 
class="cmti-12">. The set of</span>
<span 
class="cmti-12">subderivatives of</span> \(f\) <span 
class="cmti-12">at</span> \(p\) <span 
class="cmti-12">is called the </span><span 
class="cmbxti-10x-x-120">subdi&#xFB00;erential</span><span 
class="cmti-12">.</span>
</p>
  </div>
  <div class="newtheorem">
<!--l. 77--><p class="noindent" ><span class="head">
<a 
 id="x1-2006r6"></a>
<span 
class="cmbx-12">Lemma 2.6.</span>  </span> <span 
class="cmti-12">If</span> \(f\) <span 
class="cmti-12">is convex, the subdi&#xFB00;erential at</span> \(p\) <span 
class="cmti-12">is</span> \([a,b]\) <span 
class="cmti-12">where</span> \(a = \lim _{x \to p^-}\frac{f(x)-f(p)}{x-p}\)<span 
class="cmti-12">,</span> \(b = \lim _{x \to p^+}\frac{f(x)-f(p)}{x-p}\)<span 
class="cmti-12">. Moreover, these limits exist and</span> \(a \leq b\)<span 
class="cmti-12">. If</span>
\(f\) <span 
class="cmti-12">is strictly convex, then</span> \(f(x)-f(y)&gt; c(x-y)\) <span 
class="cmti-12">when</span> \(x \neq y\) <span 
class="cmti-12">if c is a derivative.</span>
</p>
  <div class="proof">
<!--l. 80--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>WLOG, \(f(p)=p=0\). Setting \(y = 0\) in the de&#xFB01;nition of convex, we get \(f(tx) \leq tf(x)\), so \(\frac{f(x)}{x}\) is increasing for all \(x&gt;0\), and \(x&lt;0\).
Now by convexity again, \(2f(0) \leq f(\ee )+f(-\ee )\) so \(\frac{f(-\ee )}{-\ee }\geq \frac{f(\ee )}{\ee }\). Thus the limits \(a,b\) exist, and are &#xFB01;nite, and it is clear that \([a,b]\) is the
subdi&#xFB00;erential. Running through the proof for strictly convex \(f\) shows \(\frac{f(x)}{x}\) is strictly increasing,
so that the strict inequality holds. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 84--><p class="noindent" ><span class="head">
<a 
 id="x1-2007r7"></a>
<span 
class="cmbx-12">Lemma 2.7.</span>  </span><span 
class="cmti-12">If</span> \(f\) <span 
class="cmti-12">is</span> \(\cC ^2\)<span 
class="cmti-12">, with</span> \(f''\geq 0\)<span 
class="cmti-12">, it is convex. If</span> \(f''&gt;0\)<span 
class="cmti-12">, it is strictly convex.</span>
</p>
  <div class="proof">
<!--l. 87--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By Taylor&#x2019;s Theorem, one computes \[tf(x)+(1-t)f(y)-f(tx+(1-t)y) = t(f(x)-f(tx+(1-t)y)+(1-t)(f(y)-f(tx+(1-t)y))\] \[=tf''(c)(1-t)^2(y-x)^2/2+(1-t)f''(c')t^2(y-x)^2/2\], which satis&#xFB01;es the correct inequalities by
assumption. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 93--><p class="noindent" ><span class="head">
<a 
 id="x1-2008r8"></a>
<span 
class="cmbx-12">Theorem 2.8 </span>&#x0028;Jensen&#x2019;s Inequality&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \((\Omega ,\mu )\) <span 
class="cmti-12">is a probability measure,</span> \(f: \RR \to \RR \) <span 
class="cmti-12">a convex function,</span> \(g\) <span 
class="cmti-12">a</span>
\(\mu \)<span 
class="cmti-12">-integrable function, then</span> \(f(\int _\Omega g d\mu ) \leq \int _\Omega f \circ g d\mu \)<span 
class="cmti-12">. If</span> \(f\) <span 
class="cmti-12">is strictly convex, equality holds i&#xFB00;</span> \(g\) <span 
class="cmti-12">takes constant value on a</span>
<span 
class="cmti-12">set of measure</span> \(1\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 97--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>De&#xFB01;ne \(x_0 = \int _\Omega gd\mu \). By Lemma <a 
href="#x1-2006r6">2.6<!--tex4ht:ref: subdifferential --></a> for \(c\), there is \(a,b\) so \(ax+b\geq f(x)\), \(ax_0+b = f(x_0)\). Then \(f(\int _\Omega gd\mu ) = f(x_0) = ax_0+b = \int _\Omega (ag+b)d\mu \leq \int _\Omega f\circ g d\mu \). Equality then holds i&#xFB00; \(ag+b \neq f\circ g\) on a set of
measure 0. If \(f\) is strictly convex, by Lemma <a 
href="#x1-2006r6">2.6<!--tex4ht:ref: subdifferential --></a>, this holds i&#xFB00; \(g\) takes constant value on a set
of measure \(1\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 101--><p class="indent" >  Now that we have this inequality, we can prove many inequalities more quickly, especially
exploiting the convexity/concavity of functions like \(\log \).
</p>
                                                                                         
                                                                                         
  <div class="newtheorem">
<!--l. 103--><p class="noindent" ><span class="head">
<a 
 id="x1-2009r9"></a>
<span 
class="cmbx-12">De&#xFB01;nition 2.9.</span>  </span><span 
class="cmti-12">The </span><span 
class="cmbxti-10x-x-120">weighted power mean </span><span 
class="cmti-12">with exponent</span> \(p\) <span 
class="cmti-12">is the function</span> \(M_p(x_1,\dots ,x_n) = (\sum w_ix_i^p)^{1/p}\) <span 
class="cmti-12">where</span> \(x_i\) <span 
class="cmti-12">are</span>
<span 
class="cmti-12">positive reals, and</span> \(w_i\) <span 
class="cmti-12">are weights summing to 1.</span>
</p>
  </div>
<!--l. 107--><p class="indent" >  In particular, \(M_\infty \) is the maximum, \(M_2\) the square mean, \(M_1\) the arithmetic mean, \(M_0\) the geometric
mean, \(M_{-1}\) the harmonic mean, and \(M_{-\infty }\) the minimum. The only one which isn&#x2019;t so obvious
is \(M_0\), but to see this, by L&#x2019;Hopital&#x2019;s Rule and continuity of the exponential function,
\[\lim _{p \to 0}\big (\sum w_ix_i^p\big )^{\frac{1}{p}}=\lim _{p \to 0}e^{\log \big (\sum w_ix_i^p\big )^{\frac{1}{p}}} =e^{\lim _{p \to 0}\frac{\log \big (\sum w_ix_i^p\big )}{p}} = e^{\lim _{p \to 0}\frac{\sum w_ix_i^p \log (x_i)}{\big (\sum w_ix_i^p\big )}}\]\[ = e^{\sum w_i\log (x_i)} = \prod x_i^{w_i} \]
</p><!--l. 110--><p class="indent" >  The following generalizes Theorem <a 
href="#x1-2002r2">2.2<!--tex4ht:ref: amgm --></a>.
</p>
  <div class="newtheorem">
<!--l. 111--><p class="noindent" ><span class="head">
<a 
 id="x1-2010r10"></a>
<span 
class="cmbx-12">Theorem 2.10 </span>&#x0028;Power Mean Inequality&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">Let</span> \(x_i\) <span 
class="cmti-12">be positive reals,</span> \(w_i\) <span 
class="cmti-12">weights. Then</span> \(p &lt; q \implies M_p \leq M_q\)<span 
class="cmti-12">, with equality</span>
<span 
class="cmti-12">holding i&#xFB00; the</span> \(x_i\) <span 
class="cmti-12">with positive weights are equal.</span>
</p>
  <div class="proof">
<!--l. 114--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>First we will prove the inequality for the cases \(p=0\),\(q=0\). By Jensen&#x2019;s inequality using concavity
of \(\log \), \(\log \prod x_i^{w_i} = \sum \frac{w_i}{q}\log x_i^q \leq \frac{\log (\sum w_ix_i^q)}{q}\) for \(p&gt;0\) and \(q=0\) case is similar. Now it su&#xFB03;ces to prove the inequality when \(pq&gt;0\), and note that
the \(p&gt;0\) and \(p&lt;0\) cases are equivalent since \(\big (\sum w_ix_i^{-p}\big )^{\frac{1}{-p}} = \frac{1}{\big (\sum w_i(\frac{1}{x_i})^{p}\big )^\frac{1}{p}}\). Now note \(x^{\frac{p}{q}}\) is concave, so \((\sum w_ix_i^p)^\frac{1}{p} = (\sum w_i(x_i^q)^{\frac{p}{q}})^\frac{1}{p} \leq (\sum w_ix_i^q)^{\frac{p}{q}\frac{1}{p}} =(\sum w_ix_i^q)^{\frac{1}{q}}\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 118--><p class="indent" >  We can also quickly get the H&#x00F6;lder inequality.
</p>
  <div class="newtheorem">
<!--l. 120--><p class="noindent" ><span class="head">
<a 
 id="x1-2011r11"></a>
<span 
class="cmbx-12">Lemma 2.11.</span>  </span> \(x,y&gt;0 \implies xy \leq \frac{x^p}{p}+\frac{x^q}{q}\) <span 
class="cmti-12">with</span> \(\frac{1}{p}+\frac{1}{q}=1\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
                                                                                         
                                                                                         
<!--l. 123--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We give two proofs. Jensen&#x2019;s inequality gives \(\log (xy) = \frac{\log (x^p)} p + \frac{\log (y^q)} q \leq \log (\frac{x^p}{p}+\frac{y^q}{q})\). Alternatively, it is equivalent to prove \(x^\frac{1}{p}y^\frac{1}{q}\leq \frac{x}{p} + \frac{y}{q}\),
which is homogeneous so WMA y = 1, in which case, we can optimize \(x\) to get the inequality.
<span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 127--><p class="noindent" ><span class="head">
<a 
 id="x1-2012r12"></a>
<span 
class="cmbx-12">Theorem 2.12 </span>&#x0028;H&#x00F6;lder Inequality&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">Let</span> \((\Omega ,\Sigma ,\mu )\) <span 
class="cmti-12">be a measure space,</span> \(\frac{1}{p}+\frac{1}{q}=1\) <span 
class="cmti-12">and</span> \(f,g\) <span 
class="cmti-12">measurable functions.</span>
<span 
class="cmti-12">Then</span> \(\Vert fg\Vert _1 \leq \Vert f\Vert _p\Vert g\Vert _q\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 131--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By Lemma <a 
href="#x1-2011r11">2.11<!--tex4ht:ref: holderlemma --></a>, \[\Vert fg\Vert _1 = \int _\Omega |fg|d\mu =\int _\Omega t|f|t^{-1}|g|d\mu \leq \int _\Omega t^p\frac{|f|^p}{p}d\mu + \int _\Omega t^{-q}\frac{|g|^q}{q}d\mu = \frac{t^p}{p}\Vert f\Vert _p^p + \frac{t^{-q}}{q}\Vert g\Vert _q^q\] We optimize \(t\) to get \(t = \frac{\Vert g \Vert _q^{\frac{q}{p+q}}}{\Vert f\Vert _p^{\frac{p}{p+q}}}\), and plugging this in and simplifying yields
the inequality. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 137--><p class="indent" >  The H&#x00F6;lder inequality can establish that \(L^p\) spaces are normed vector spaces. The triangle
inequality is the hard part of the proof.
</p>
  <div class="newtheorem">
<!--l. 139--><p class="noindent" ><span class="head">
<a 
 id="x1-2013r13"></a>
<span 
class="cmbx-12">Lemma 2.13.</span>  </span>\(|x+y|^p \leq 2^{p-1}(|x|^p+|y|^p)\) <span 
class="cmti-12">for</span> \(p&gt;1\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 142--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>\(f(x)=x^p\) is convex, so \(|\frac{x+y}{2}|^p\leq \frac 1 2 (|x|^p+|y|^p)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 146--><p class="noindent" ><span class="head">
<a 
 id="x1-2014r14"></a>
<span 
class="cmbx-12">Theorem 2.14 </span>&#x0028;Minkowski&#x2019;s Inequality&#x0029;<span 
class="cmbx-12">.</span>  </span>\(\Vert f+g\Vert _p \leq \Vert f \Vert _p+\Vert g \Vert _p\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 149--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We give two proofs, both which use H&#x00F6;lder inequality. By the Lemma, \(\Vert f+g\Vert _p\) is &#xFB01;nite if \(\Vert f\Vert _p, \Vert g\Vert _p\)
are. Then by H&#x00F6;lder inequality, \[\Vert f+g \Vert _p^p = \int |f+g|^p \leq \int |f||f+g|^{p-1}+|g||f+g|^{p-1} \]\[ \leq (\Vert f\Vert _p +\Vert g\Vert _p)\Vert (f+g)^{p-1}\Vert _{\frac{p}{p-1}} = (\Vert f\Vert _p +\Vert g\Vert _p)\Vert f+g\Vert _p^{p-1}\].
</p><!--l. 153--><p class="indent" >  For the second proof, we claim \(\Vert f\Vert _p = \sup _{\Vert h\Vert _q=1}\Vert fh\Vert _1\), with which the theorem follows from the triangle inequality
for \(L^1\). The \(\geq \) follows from the H&#x00F6;lder inequality, and the \(\geq \) follows from setting \(h\) to be \(f^{p-1}\) divided
by its norm. Note that this proof also shows the duality between \(L^p\) and \(L^q\) &#x0028;indeed they are duals
as Banach spaces&#x0029;. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">3. </span> <a 
 id="x1-30003"></a>Topology of \(\RR \)</h3>
  <div class="newtheorem">
<!--l. 158--><p class="noindent" ><span class="head">
<a 
 id="x1-3001r1"></a>
<span 
class="cmbx-12">Lemma 3.1.</span>  </span> <span 
class="cmti-12">An in&#xFB01;nite subset</span> \(X\) <span 
class="cmti-12">of a compact Hausdor&#xFB00; space</span> \(Y\) <span 
class="cmti-12">has a limit point.</span>
</p>
  <div class="proof">
<!--l. 162--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If not, \(X\) is closed, and is a countable compact discrete space, which is a contradiction.
<span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
                                                                                         
                                                                                         
  </div>
  <div class="newtheorem">
<!--l. 166--><p class="noindent" ><span class="head">
<a 
 id="x1-3002r2"></a>
<span 
class="cmbx-12">Proposition 3.2 </span>&#x0028;Baire&#x2019;s Category Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">A   locally   compact   Hausdor&#xFB00;   space   or</span>
<span 
class="cmti-12">complete metric space</span> \(X\) <span 
class="cmti-12">is a Baire space.</span>
</p>
  <div class="proof">
<!--l. 170--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>First let&#x2019;s treat locally compact Hausdor&#xFB00;. Let \(X_n\) be closed sparse subsets of \(X\), and \(U_0\) an
open set in \(X\). We can inductively produce \(\bar{U_i} \subset U_{i-1}\) that avoid \(X_1,\dots ,X_i\) by regularity. But then \(\cap _i\bar{U_i}\) is nonempty,
so there is a point that avoids all \(U_i\). Similarly in the complete metric space case, we can choose
the \(U_i\) in the same way with the condition that the diameter of \(U_i\) is less than \(1/n\). Then again the
intersection must be a single point by completeness. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 174--><p class="noindent" ><span class="head">
<a 
 id="x1-3003r3"></a>
<span 
class="cmbx-12">Theorem 3.3.</span>  </span> <span 
class="cmti-12">The unit cube</span> \(I^n\) <span 
class="cmti-12">is compact.</span>
</p>
  <div class="proof">
<!--l. 178--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Suppose we have a covering with no &#xFB01;nite subcover. Then we can divide the unit
cube into \(2^n\) pieces of half the size, and at least one of these must have the same property. We
can keep doing this, getting a chain of cubes contained within one another with diameter
approaching \(0\) such that this cover has no &#xFB01;nite subcover for any of these. But the intersection
of these has diameter \(0\), but contains exactly \(1\) point by taking the limiting point of a sequence
of points in each of the cubes in the chain. Taking an open set surrounding this point, we
get a contradiction as in&#xFB01;nitely many cubes must be in the open set. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
                                                                                         
                                                                                         
  </div>
  <div class="newtheorem">
<!--l. 182--><p class="noindent" ><span class="head">
<a 
 id="x1-3004r4"></a>
<span 
class="cmbx-12">Corollary 3.4 </span>&#x0028;Heine-Borel&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">A subset of</span> \(\RR ^n\) <span 
class="cmti-12">is compact i&#xFB00; it is closed and bounded.</span>
</p>
  <div class="proof">
<!--l. 185--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If a subset is compact, it is closed as \(\RR ^n\) is Hausdor&#xFB00;, and bounded by looking at the
cover given by open balls around the origin. Conversely if a subset is closed and bounded, it
is a closed subset of a large unit cube, hence is compact by Theorem <a 
href="#x1-3003r3">3.3<!--tex4ht:ref: cubecompact --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 189--><p class="noindent" ><span class="head">
<a 
 id="x1-3005r5"></a>
<span 
class="cmbx-12">Corollary 3.5 </span>&#x0028;Bolzano-Weierstrass&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">A  subset  of</span>  \(\RR ^n\)  <span 
class="cmti-12">is  compact  i&#xFB00;  any  sequence  has  a</span>
<span 
class="cmti-12">convergent subsequence.</span>
</p>
  <div class="proof">
<!--l. 192--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>&#x2018; Suppose any sequence has a convergent subsequence. Then the subset must be
bounded or else we could take an increasingly large sequence. It must also be closed or else we
could take points in decreasingly small neighborhoods of a limit point. The converse follows
from Lemma <a 
href="#x1-3001r1">3.1<!--tex4ht:ref: comphaus --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 197--><p class="noindent" ><span class="head">
<a 
 id="x1-3006r6"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Lemma 3.6 </span>&#x0028;Minimum-Maximum Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">A map</span> \(f:S \to \RR \) <span 
class="cmti-12">from compact</span> \(S\) <span 
class="cmti-12">has a min and</span>
<span 
class="cmti-12">max.</span>
</p>
  <div class="proof">
<!--l. 200--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>The  image  is  compact,  hence  closed  and  bounded,  so  attains  its  supremum  and
in&#xFB01;mum. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 204--><p class="noindent" ><span class="head">
<a 
 id="x1-3007r7"></a>
<span 
class="cmbx-12">Theorem 3.7.</span>  </span><span 
class="cmti-12">All norms on &#xFB01;nite dimensional</span> \(\RR \)<span 
class="cmti-12">-vector spaces are equivalent.</span>
</p>
  <div class="proof">
<!--l. 208--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We will show every norm is equivalent to \(\Vert \cdot \Vert _\infty \). To do this, let \(\Vert \cdot \Vert \) be any norm, and we will
show it is continuous with respect to \(\Vert \cdot \Vert _\infty \). In particular, if \(M = \max _i\Vert \delta _i \Vert \) we have \(\Vert x\Vert \leq \sum _1^n\Vert x_i\delta _i\Vert \leq M\sum _1^n|x_i| \leq Mn\Vert x\Vert _\infty \), so we have \(|\Vert x \Vert - \Vert a \Vert | \leq \Vert x-a\Vert \leq Mn\Vert x-a \Vert _\infty \) so indeed it is
continuous.
</p><!--l. 210--><p class="indent" >  The set \(X = \{x|\Vert x \Vert _\infty = 1\}\) is compact, so its image from \(\Vert \cdot \Vert \) by Lemma <a 
href="#x1-3006r6">3.6<!--tex4ht:ref: minmax --></a> has a minimum \(m\) and a maximum \(M\).
Thus \(m\) and \(M\) give the bounds we need for equivalence of norms. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 214--><p class="noindent" ><span class="head">
<a 
 id="x1-3008r8"></a>
<span 
class="cmbx-12">Lemma 3.8.</span>  </span> <span 
class="cmti-12">There is no retraction from</span> \(D^n \to S^n\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 217--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>Look at \(H^{n-1}\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 221--><p class="noindent" ><span class="head">
<a 
 id="x1-3009r9"></a>
<span 
class="cmbx-12">Proposition 3.9 </span>&#x0028;Intermediate Value Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">A map from</span> \(D^n\) <span 
class="cmti-12">to itself that is the identity</span>
<span 
class="cmti-12">on the boundary is surjective.</span>
</p>
  <div class="proof">
<!--l. 225--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If not, we can deformation retract away from the missing point to the boundary, to
yield a retraction of \(D^n\) onto \(S^{n-1}\), contradicting Lemma <a 
href="#x1-3008r8">3.8<!--tex4ht:ref: noretract --></a>. In the case \(n=1\) this is looking at the
connected component. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 229--><p class="noindent" ><span class="head">
<a 
 id="x1-3010r10"></a>
<span 
class="cmbx-12">Proposition 3.10 </span>&#x0028;Brouwer Fixed Point Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">A map</span> \(f:D^n \to D^n\) <span 
class="cmti-12">has a &#xFB01;xed point.</span>
</p>
  <div class="proof">
<!--l. 232--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If not, we can consider the line going from \(f(x)\) to \(x\) and produce a retraction \(g\) that is the
intersection &#x0028;on the \(x\) side&#x0029; of this line with the boundary, contradicting Lemma <a 
href="#x1-3008r8">3.8<!--tex4ht:ref: noretract --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 236--><p class="noindent" ><span class="head">
                                                                                         
                                                                                         
<a 
 id="x1-3011r11"></a>
<span 
class="cmbx-12">Theorem 3.11.</span>  </span> <span 
class="cmti-12">For any embedding</span> \(i:D^k \to S^n\)<span 
class="cmti-12">,</span> \(S^n-i(D^k)\) <span 
class="cmti-12">has trivial</span> \(\tilde{H_*}\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 239--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We induct on \(k\), for \(k =0\) we just have \(S^n-i(D_0) = \RR ^n\). For the induction step, we replace \(D^k\) with \(I^k\). Now
we split \(I^k\) into two halves \(I^{k-1}\times [0,\frac{1}{2}],I^{k-1}\times [\frac{1}{2},1]\) and by Mayer-Vietoris and induction we have \(\tilde{H}_*(S^n-I^k) \hookrightarrow \tilde{H}_*(I^{k-1}\times [0,\frac{1}{2}])\oplus \tilde{H}_*(I^{k-1}\times [\frac{1}{2},1])\) is an isomorphism.
If there were a nontrivial &#x0028;reduced&#x0029; cycle \(\alpha \) in \(H_*(S^n-I^k)\) it would land nontrivially in one of the two
summands on the right, and we could repeatedly cut these up into smaller intervals, in the
limit landing in \(H_*(S^n-I^{k-1})\) in which we know it would be a boundary of a chain \(\beta \). But then as \(\beta \) is
compact and covered by our cuts, it is in one of the cuts, a contradiction. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 243--><p class="noindent" >The next theorem has the Jordan Curve Theorem as a special case:
</p>
  <div class="newtheorem">
<!--l. 244--><p class="noindent" ><span class="head">
<a 
 id="x1-3012r12"></a>
<span 
class="cmbx-12">Theorem 3.12.</span>  </span> <span 
class="cmti-12">For any embedding</span> \(i:S^k \to S^n,n&gt;k\) <span 
class="cmti-12">we have</span> \(\tilde{H}_i (S^n-i(S^k)) = \ZZ ^{\delta _{i,n-k-1}}\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 248--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We  induct  on  \(k\),  for  \(k=0\)  we  have  \(S^n-i(S^0) \simeq S^{n-1}\).  Now  to  induct  we  split  \(S^k\)  into  to  disks  and  use
Mayer-Vietoris and Theorem <a 
href="#x1-3011r11">3.11<!--tex4ht:ref: diskinsphere --></a> to give \(H_{m-1}(S^n-i(S^k))\cong H_{m}(S^n-i(S^{k-1})).\) <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 252--><p class="noindent" ><span class="head">
<a 
 id="x1-3013r13"></a>
<span 
class="cmbx-12">Theorem 3.13 </span>&#x0028;Invariance of Domain&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">An injective map</span> \(\RR ^n \to \RR ^n\) <span 
class="cmti-12">is open.</span>
                                                                                         
                                                                                         
</p>
  <div class="proof">
<!--l. 255--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By Theorem <a 
href="#x1-3012r12">3.12<!--tex4ht:ref: jordancurve --></a>, the boundary of an embedding \(D^n \to \RR ^n\) separates \(\RR ^n\) into two components,
and since the boundary is closed, the interior of \(D^n\) is an connected component of this separation
that is open in \(\RR ^n\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 259--><p class="noindent" ><span class="head">
<a 
 id="x1-3014r14"></a>
<span 
class="cmbx-12">Lemma 3.14 </span>&#x0028;Lebesgue Number Lemma&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">Any open cover of a compact metric space</span> \(X\) <span 
class="cmti-12">has</span>
<span 
class="cmti-12">a</span> \(\delta \) <span 
class="cmti-12">&#x0028;Lebesgue number&#x0029; so that every</span> \(\delta \) <span 
class="cmti-12">ball is in some open set.</span>
</p>
  <div class="proof">
<!--l. 263--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Take a &#xFB01;nite subcover \(U_1, \dots , U_n\), and consider the map \(f:X \to \RR \), \(f(x) = \sum d(x,X-U_i)\). Now this function attains a minimum
\(\delta \), but \(\delta \) cannot be \(0\) as the \(U_i\) cover \(X\). Then \(\delta \) is a Lebesgue number. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 267--><p class="noindent" ><span class="head">
<a 
 id="x1-3015r15"></a>
<span 
class="cmbx-12">Proposition 3.15.</span>  </span><span 
class="cmti-12">A  continuous  map</span>  \(f:S \to T\)  <span 
class="cmti-12">where</span>  \(S,T\)  <span 
class="cmti-12">are  metric  spaces,  and</span>  \(S\)  <span 
class="cmti-12">is  compact  is</span>
<span 
class="cmti-12">uniformly continuous.</span>
</p>
  <div class="proof">
<!--l. 271--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Take the preimages of \(\ee \) balls around each point in \(T\) to make a cover of \(S\). The Lebesgue
number of this cover is a uniform \(\delta \) for this \(\ee \). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 275--><p class="noindent" ><span class="head">
<a 
 id="x1-3016r16"></a>
<span 
class="cmbx-12">Theorem 3.16 </span>&#x0028;Uniform Limit Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">A uniform limit of continuous functions to a</span>
<span 
class="cmti-12">metric space is continuous.</span>
</p>
  <div class="proof">
<!--l. 279--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Let the sequence be \(f_n\) and the limit be \(f\). We need that for any \(\ee &gt;0\) and any \(x \in X\) there is a
neighborhood \(x \in U_\ee \) such that \(d(f(x),f(y))&lt;\ee \) when \(x,y \in U_\ee \). We can choose large enough \(n\) so we have \(\forall x,d(f(x),f_n(x))&lt;\ee \), and a neighborhood
\(U\) satisfying \(\ee \) for \(f_n\) Then we have in \(U\): \[ d(f(x),f(y)) \leq d(f(x),f_n(x))+d(f_n(x),f_n(y))+d(f_n(y),f(y)) &lt; \ee +\ee +\ee =3\ee \] so we are done. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 288--><p class="noindent" ><span class="head">
<a 
 id="x1-3017r17"></a>
<span 
class="cmbx-12">Theorem 3.17.</span>  </span> <span 
class="cmti-12">If a sequence of continuous functions</span> \(f_n\) <span 
class="cmti-12">from a compact space</span> \(X\) <span 
class="cmti-12">to a metric</span>
<span 
class="cmti-12">space</span> \(Y\) <span 
class="cmti-12">converge to a function</span> \(f\)<span 
class="cmti-12">, they uniformly converge to that function, which is continuous</span>
<span 
class="cmti-12">by Theorem </span><a 
href="#x1-3016r16"><span 
class="cmti-12">3.16</span><!--tex4ht:ref: uniflimitthm --></a><span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 292--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>For any \(\ee &gt;0\), there is a neighborhood around each point \(x\in X\), \(U_x\) such that \(f(y)\) is within \(\ee /2\) of \(f(x)\) for any
\(y \in U_x\). Then make a &#xFB01;nite cover of these \(U_x\) and take the maximum \(N\) for each of the corresponding
points such that \(f_N\) is within \(\frac{\ee }{2}\) for these points. Then by the triangle inequality every point
simultaneously satis&#xFB01;es \(|f(x)-f_N(x)|\leq \ee \) for large enough \(N\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
                                                                                         
                                                                                         
  </div>
  <div class="newtheorem">
<!--l. 296--><p class="noindent" ><span class="head">
<a 
 id="x1-3018r18"></a>
<span 
class="cmbx-12">Theorem 3.18 </span>&#x0028;Ascoli&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(f_n:X \to Y\) <span 
class="cmti-12">form a uniformly equicontinuous sequence of maps between compact</span>
<span 
class="cmti-12">metric spaces</span> \(X\) <span 
class="cmti-12">and</span> \(Y\)<span 
class="cmti-12">, there is a uniformly convergent subsequence.</span>
</p>
  <div class="proof">
<!--l. 299--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>\(X\) has a countable dense subset \(x_n\), so starting with \(x_1\) we can look at \(f_i(x_1)\), which has a limit
point by Lemma <a 
href="#x1-3001r1">3.1<!--tex4ht:ref: comphaus --></a>. This gives a subsequence which converges at \(x_1\). We can then inductively
produce more subsequences for each \(x_n\) and take a diagonal sequence \(g_n\). Every compact metric
space is complete by Lemma <a 
href="#x1-3001r1">3.1<!--tex4ht:ref: comphaus --></a>, so it su&#xFB03;ces to show that \(g_n\) is uniformly Cauchy. To do so,
&#xFB01;x \(\ee &gt;0\), and cover \(X\) with &#xFB01;nitely many neighborhoods around the \(x_i\) so each \(f_n\) varies at most \(\frac{\ee }{3}\) in the
neighborhood. Each neighborhood contains an \(x_i\) so choose the maximum of the \(N\)s so for \(j\geq N\) the \(g_j(x_i)\)
di&#xFB00;er by at most \(\frac{\ee }{3}\). Then for any \(x \in X\), \(j,k&gt;N\) we have \(d(g_j(x),g_k(x)) \leq d(g_j(x),g_j(x_i)) + d(g_j(x_i),g_k(x_i)) + d(g_k(x_i),g_k(x)) &lt; \ee \). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">4. </span> <a 
 id="x1-40004"></a>Derivatives</h3>
<!--l. 304--><p class="noindent" >Let \(\FF \) from now on denote \(\CC \) or \(\RR \).
</p>
  <div class="newtheorem">
<!--l. 305--><p class="noindent" ><span class="head">
<a 
 id="x1-4001r1"></a>
<span 
class="cmbx-12">Proposition 4.1.</span>  </span> <span 
class="cmti-12">A function</span> \(f:\RR ^n \to \RR \) <span 
class="cmti-12">that has a local extremum at a point</span> \(a \in U\) <span 
class="cmti-12">and is di&#xFB00;erentiable</span>
<span 
class="cmti-12">there has a critical point.</span>
</p>
  <div class="proof">
<!--l. 309--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>It su&#xFB03;ces to consider \(n=1\), as this implies each of the \(\partial _if(a)\) in the general case is \(0\). In this
case, note that if \(|f'(a)|&gt;0\) then for small enough \(\ee \) we have \(\frac{|\ee |}{|h|}&lt;|f'(a)|\) so the function decreases on one side and
increases on the other. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 313--><p class="noindent" ><span class="head">
<a 
 id="x1-4002r2"></a>
<span 
class="cmbx-12">Proposition 4.2 </span>&#x0028;Mean Value Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f,g:[a,b]\to \RR \) <span 
class="cmti-12">are continuous, and di&#xFB00;erentiable on the interior,</span>
<span 
class="cmti-12">then there is a</span> \(c\) <span 
class="cmti-12">so that </span>\[ (g(b)-g(a))f'(c)=(f(b)-f(a))g'(c) \]
</p>
  <div class="proof">
<!--l. 319--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Consider the function \(h = (g(b)-g(a))f(x)-(f(b)-f(a))g(x)\). \(h(a)=g(b)f(a)-f(b)g(a)=h(b)\), so either \(h\) is constant for which any point works, or it attains
some maximum/minimum on the interior of the interval, which by Proposition <a 
href="#x1-4001r1">4.1<!--tex4ht:ref: extrema --></a> yields a
point \(c\) with \(h'(c) = 0\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 322--><p class="indent" >  Note that the Mean Value Theorem is most often used in the case where \(g(x) = x\).
</p><!--l. 324--><p class="indent" >  The chain rule is the theorem that in the category \(\Diff \), \(d\) is an endofunctor that takes a manifold to
its tangent space, and a map \(f\) to a map \(df\) &#x0028;the pushforward, total derivative, or derivative&#x0029; on
tangent spaces. In the case of a map between open sets of \(\FF ^n\), the tangent space is canonically
identi&#xFB01;ed with \(\FF ^n \times \FF ^n\). Here is the classical statement:
</p>
  <div class="newtheorem">
<!--l. 325--><p class="noindent" ><span class="head">
<a 
 id="x1-4003r3"></a>
<span 
class="cmbx-12">Theorem 4.3 </span>&#x0028;Chain Rule&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">Suppose we have</span> \(U \subset \FF ^l, V \subset \FF ^m, W \subset \FF ^n\)<span 
class="cmti-12">, and there are maps</span> \(f:U \to V, g:V \to W\) <span 
class="cmti-12">such that</span> \(f\) <span 
class="cmti-12">is di&#xFB00;erentiable at</span>
\(a\)<span 
class="cmti-12">, and</span> \(g\) <span 
class="cmti-12">is di&#xFB00;erentiable at</span> \(f(a)\)<span 
class="cmti-12">. Then</span> \(g \circ f\) <span 
class="cmti-12">is di&#xFB00;erentiable and</span> \((g \circ f)'(a) = g'(f(a))f'(a)\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 328--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We de&#xFB01;ne \(k = f(a+h)-f(a)\). Then: \[ (g\circ f)(a+h)-(g\circ f)(a) = g(f(a+h))-g(f(a)) = g(f(a)+k)-g(f(a)) \] \[ = g'(f(a))k + \ee _g(k) = g'(f(a))(f(a+h)-f(a)) + \ee _g(k). = g'(f(a))(f'(a)h+\ee _f(h)) + \ee _g(k) \] \[=g'(f(a))f'(a)h + \ee \] where \(\ee = g'(f(a))\ee _f(h) + \ee _g(k)\) so it su&#xFB03;ces to show \(\frac{\Vert \ee \Vert }{\Vert h\Vert } \to 0\) as \(h \to 0\). With \(\Vert \cdot \Vert _o\) denoting the operator
norm we have: \[ \frac{\Vert \ee \Vert }{\Vert h\Vert } \leq \frac{\Vert \ g'(f(a))\Vert _o \Vert e_f(h)\Vert }{\Vert h\Vert } + \frac{\Vert e_g(k)\Vert }{\Vert k\Vert }\frac{\Vert f(a+h)-f(a)\Vert }{\Vert h \Vert } \] \[= \frac{\Vert \ g'(f(a))\Vert _o \Vert e_f(h)\Vert }{\Vert h\Vert } + \frac{\Vert e_g(k)\Vert }{\Vert k\Vert }\bigg (\frac{\Vert f'(a)\Vert _o\Vert h \Vert }{\Vert h \Vert } + \frac{\Vert \ee _f(h)\Vert }{\Vert h \Vert }\bigg ) \] which tends to \(0\) by hypothesis. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 346--><p class="noindent" ><span class="head">
<a 
 id="x1-4004r4"></a>
<span 
class="cmbx-12">Corollary 4.4 </span>&#x0028;Product &#x0026; Quotient Rules&#x0029;<span 
class="cmbx-12">.</span>  </span> \[\partial _i(f(x)g(x)) = \partial _if(x)g(x) + \partial _ig(x)f(x)\] \[\partial _i\frac{f(x)}{g(x)} = \frac{\partial _if(x)g(x)-\partial _ig(x)f(x)}{g(x)^2}\]
</p>
  <div class="proof">
<!--l. 350--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Use the chain rule on the composite \(x \mapsto (f(x),g(x)) \mapsto f(x)g(x)\) and \(x \mapsto (f(x),g(x)) \mapsto \frac{f(x)}{g(x)}\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 353--><p class="noindent" ><span class="head">
<a 
 id="x1-4005r5"></a>
<span 
class="cmbx-12">Corollary 4.5.</span>  </span><span 
class="cmti-12">If the inverse of</span> \(f\) <span 
class="cmti-12">is di&#xFB00;erentiable,</span> \((f^{-1})'(f(x)) = f'(x)^{-1}\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 356--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Apply the chain rule to \(f^{-1}\circ f\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 360--><p class="noindent" ><span class="head">
<a 
 id="x1-4006r6"></a>
<span 
class="cmbx-12">Corollary 4.6 </span>&#x0028;Mean Value Theorem Many Variables&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR ^n \to \RR \) <span 
class="cmti-12">is continuous at</span> \(a,b\) <span 
class="cmti-12">and di&#xFB00;erentiable on</span>
<span 
class="cmti-12">a neighborhood which contains the line segment strictly between</span> \(a\) <span 
class="cmti-12">and</span> \(b\)<span 
class="cmti-12">, then there is a</span> \(c\) <span 
class="cmti-12">on this line</span>
<span 
class="cmti-12">segment satisfying</span> \(f(b)-f(a) = f'(c)(b-a)\)<span 
class="cmti-12">.</span>
</p>
                                                                                         
                                                                                         
  <div class="proof">
<!--l. 363--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Consider the function \(g:[0,1] \to U\) going to the straight line between \(a\) and \(b\). By Proposition <a 
href="#x1-4002r2">4.2<!--tex4ht:ref: mvt --></a>
we have \((f\circ g)'(c) = f(b)-f(a)\), and we can use the chain rule to get \((f\circ g)'(c)= f'(g(c))g'(c)=f'(g(c))(b-a)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 367--><p class="noindent" ><span class="head">
<a 
 id="x1-4007r7"></a>
<span 
class="cmbx-12">Corollary 4.7 </span>&#x0028;Mean Value Inequality&#x0029;<span 
class="cmbx-12">.</span>  </span>  <span 
class="cmti-12">If</span>  \(f:\RR ^m \to \RR ^n\)  <span 
class="cmti-12">is  continuous  at</span>  \(a,b\)  <span 
class="cmti-12">and  di&#xFB00;erentiable  on  a</span>
<span 
class="cmti-12">neighborhood which contains the line segment strictly between</span> \(a\) <span 
class="cmti-12">and</span> \(b\)<span 
class="cmti-12">, then there is a</span> \(c\) <span 
class="cmti-12">on the</span>
<span 
class="cmti-12">line segment such that</span> \(\Vert f(b)-f(a)\Vert _2 \leq \Vert f'(c)\Vert _o \Vert b-a\Vert _2\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 371--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Let \(u\) be the unit vector in the direction of \(f(b)-f(a)\). Then let \(U(x) = u \cdot x\), so \(U\circ f\) is a real valued function to
which we can apply Corollary <a 
href="#x1-4006r6">4.6<!--tex4ht:ref: mvtmanyvar --></a>. We get \((U\circ f)(b)-(U\circ f)(a) = (U\circ f)'(c)(b-a) = U'(f(c))f'(c)(b-a) = u \cdot (f'(c)(b-a))\). Then we have via Cauchy-Schwarz inequality, \[ \Vert f(b)-f(a)\Vert _2 = |(U\circ f)(b)-(U\circ f)(a)| = |u \cdot (f'(c)(b-a))| \] \[ \leq \Vert u \Vert _2\Vert f'(c)(b-a)\Vert \leq \Vert f'(c)\Vert _o\Vert b-a\Vert _2 \]
<span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 382--><p class="noindent" ><span class="head">
<a 
 id="x1-4008r8"></a>
<span 
class="cmbx-12">Corollary 4.8.</span>  </span><span 
class="cmti-12">If</span> \(f:\RR ^m \to \RR ^n\) <span 
class="cmti-12">is di&#xFB00;erentiable in a convex bounded open set</span> \(U\)<span 
class="cmti-12">, and</span> \(\Vert f'(x)\Vert _o\) <span 
class="cmti-12">is bounded on</span> \(U\)<span 
class="cmti-12">, then</span>
\(f\) <span 
class="cmti-12">is uniformly continuous.</span>
</p>
  <div class="proof">
<!--l. 386--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Corollary <a 
href="#x1-4007r7">4.7<!--tex4ht:ref: mvineq --></a> gives \(\Vert f(x)-f(y) \Vert _2 \leq M\Vert x-y\Vert \). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 390--><p class="noindent" ><span class="head">
<a 
 id="x1-4009r9"></a>
<span 
class="cmbx-12">Corollary 4.9 </span>&#x0028;L&#x2019;H&#x00F4;pital&#x2019;s Rule&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(f,g:\RR ^n \to \RR \) <span 
class="cmti-12">are continuous, di&#xFB00;erentiable in a deleted neighborhood of</span> \(0\)<span 
class="cmti-12">,</span>
\(f(0) = g(0) = 0\)<span 
class="cmti-12">, and</span> \(\lim _{x \to 0}g'(x) \neq 0,\lim _{x \to 0}f'(x)\) <span 
class="cmti-12">exist, then </span>\[ \lim _{x \to 0}\frac{f(x)}{g(x)} = \lim _{x \to 0}\frac{f'(x)}{g'(x)} \]
</p>
  <div class="proof">
<!--l. 396--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Use Proposition <a 
href="#x1-4002r2">4.2<!--tex4ht:ref: mvt --></a> on a small interval \((-\delta ,\delta )\) around the origin, to get \(\frac{f(x)}{g(x)} = \frac{f(x)-f(0)}{g(x)-g(0)} = \frac{f'(x_0)}{g'(x_0)}\) for some \(x_0\) in the
interval. Letting \(\delta \to 0\) we are done. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 400--><p class="noindent" ><span class="head">
<a 
 id="x1-4010r10"></a>
<span 
class="cmbx-12">Theorem 4.10.</span>  </span> <span 
class="cmti-12">If</span> \(f:\FF ^m \to \FF ^n\) <span 
class="cmti-12">has all partial derivatives which are continuous near a point</span> \(a\)<span 
class="cmti-12">, then</span> \(f'(a)\)
<span 
class="cmti-12">exists.</span>
</p>
  <div class="proof">
<!--l. 404--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>It  su&#xFB03;ces  to  prove  this  when  \(n=1\)  as  being  di&#xFB00;erentiable  is  equivalent  to  being
di&#xFB00;erentiable in each component. Now &#xFB01;x an \(h\) in a small enough ball. We have by Proposition
<a 
href="#x1-4002r2">4.2<!--tex4ht:ref: mvt --></a>: \[f(a_1+h_1,\dots ,a_i+h_i,a_{i+1},\dots ,a_n) - f(a_1+h_1,\dots ,a_{i-1}+h_{i-1},a_{i},\dots ,a_n)\] \[= h_i\partial _if(a_1+h_1,\dots ,a_{i-1}+h_i, \alpha _i,a_{i+1},\dots ,a_n)\] where \(\alpha _i \in (a_i,a_i+h_i)\). Now we have \[f(a+h)-f(a) = \sum _ih_i\partial _if(a_1+h_1,\dots a_{i-1}+h_{i-1},\alpha _i,a_{i+1}+h_{i+1},\dots ,a_n) \] \[=h(\partial _if(a)) + \ee \] where \[\frac{\Vert \ee \Vert }{\Vert h\Vert } \leq \sum _i\Vert \partial _if(a_i+h_i,\dots ,\alpha _i,\dots ,a_n)-\partial _if(a)\Vert \] which tends to \(0\) as \(h \to 0\) by continuity of the partial derivatives.
<span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 418--><p class="noindent" ><span class="head">
<a 
 id="x1-4011r11"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Proposition 4.11.</span>  </span><span 
class="cmti-12">If</span> \(f:\FF ^2\to \FF \) <span 
class="cmti-12">is continuous near a point</span> \(a\)<span 
class="cmti-12">, has partials</span> \(\partial _1f,\partial _2f,\partial _1\partial _2f,\partial _2\partial _1f\) <span 
class="cmti-12">near</span> \(a\)<span 
class="cmti-12">, and the mixed partials</span> \(\partial _1\partial _2f,\partial _2\partial _1f\) <span 
class="cmti-12">are</span>
<span 
class="cmti-12">continuous near</span> \(a\)<span 
class="cmti-12">, then they are equal at</span> \(a\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 421--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By Proposition <a 
href="#x1-4002r2">4.2<!--tex4ht:ref: mvt --></a>, we have \[\partial _1\partial _2f(x'',y'') = \partial _2f(x+h,y'')-\partial _2f(x,y'') \]\[= f(x+h,y+k)-f(x+h,y)-f(x,y+k)+f(x,y)\]\[ = f(x+h,y+k)-f(x,y+k)-f(x+h,y)+f(x,y)\]\[ = \partial _1f(x',y+k)-\partial _1f(x',y) = \partial _2\partial _1f(x',y') \] so letting \(k,h \to 0\) by continuity we are done. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 428--><p class="noindent" ><span class="head">
<a 
 id="x1-4012r12"></a>
<span 
class="cmbx-12">Proposition 4.12 </span>&#x0028;One-variable Taylor Expansion&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(f:\RR \to \RR \) <span 
class="cmti-12">has continuous partial derivatives up to</span> \(k+1^{th}\)
<span 
class="cmti-12">order near</span> \(a\)<span 
class="cmti-12">, then for small enough</span> \(h\) <span 
class="cmti-12">we have </span>\[f(a+h) = \sum _0^k\frac{1}{i!}f^{(i)}(a)h^i + \frac{1}{(k+1)!}f^{(k+1)}(\alpha )h^{k+1} \] <span 
class="cmti-12">with</span> \(\alpha \in (a,a+h)\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 434--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Let \(R(x)\) be \(f(a+x)-\sum _0^k\frac{1}{i!}f^{(i)}(a)x^i - \frac{1}{(k+1)!}cx^{h+1}\) such that \(c\) is chosen so that \(R(h)=0\). We want to show \(c = f^{(k+1)}(\alpha )\) as above. To do this, use
Proposition <a 
href="#x1-4002r2">4.2<!--tex4ht:ref: mvt --></a> many times, &#xFB01;rst on the fact that \(R(0)=R(h) = 0\) to get a \(a_1\) where \(R'(a_1)=0\), and then repeating this
on \(R'(0)=R'(a_1)=0\). Then &#xFB01;nally we will have a \(a_{k+1}\) with \(R^{(k+1)}(a_{k+1}) = 0\), at which point taking the \(k+1^{th}\) derivative yields \(0=R^{(k+1)}(a_k) = f^{(k+1)}(a+a_{k+1})-c\), so we
are done. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 438--><p class="noindent" ><span class="head">
<a 
 id="x1-4013r13"></a>
<span 
class="cmbx-12">Corollary 4.13 </span>&#x0028;Many-variable Taylor Expansion&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(f:\RR ^n \to \RR \) <span 
class="cmti-12">has continuous partial derivatives</span>
<span 
class="cmti-12">up to</span> \(k+1^{th}\) <span 
class="cmti-12">order near</span> \(a\)<span 
class="cmti-12">, for small enough</span> \(h\) <span 
class="cmti-12">we have </span>\[f(a+h) = \sum _0^k\sum _{j_1,\dots ,j_i = 1}^n\partial _{j_1}\dots \partial _{j_i}f(a)\prod _{l=1}^ih_l + \sum _{j_1,\dots ,j_{k+1} = 1}^n\partial _{j_1}\dots \partial _{j_{k+1}}f(\alpha )\prod _{l=1}^{k+1}h_l\] <span 
class="cmti-12">with</span> \(\alpha \) <span 
class="cmti-12">in the line segment between</span> \(a\) <span 
class="cmti-12">and</span>
\(a+h\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 443--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>Apply the chain rule to extend the one-variable case by composing \(f\) with the line
between \(a\) and \(a+h\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">5. </span> <a 
 id="x1-50005"></a>Series</h3>
  <div class="newtheorem">
<!--l. 450--><p class="noindent" ><span class="head">
<a 
 id="x1-5001r1"></a>
<span 
class="cmbx-12">Lemma 5.1 </span>&#x0028;Raabe&#x2019;s Test&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">For the series</span> \(\sum _0^\infty c_n\)<span 
class="cmti-12">,</span> \(c_i \in \RR \) <span 
class="cmti-12">if</span> \(R_n = n(1-\frac{c_{n+1}}{c_n})\) <span 
class="cmti-12">is larger than</span> \(R&gt;1\) <span 
class="cmti-12">for su&#xFB03;ciently large</span> \(n\)<span 
class="cmti-12">, the series is</span>
<span 
class="cmti-12">absolutely convergent.</span>
</p>
  <div class="proof">
<!--l. 453--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>WLOG \(c_n \geq 0\). Then note that \(R_n-1 = \frac{1}{c_n}(c_n(n-1)-c_{n+1}n)\) so \(0 \leq c_n = \frac{c_n(n-1)-c_{n+1}n}{R_n-1}\). By hypothesis \(R_n-1\) stays away from \(0\) for large \(n\) so it su&#xFB03;ces
to show \(c_n(n-1)-c_{n+1}n\), which is positive for large \(n\), converges. But \(\sum _1^mc_n(n-1)-c_{n+1}n = -c_{m+1}m\) is monotonically increasing for large \(m\)
and bounded above by \(0\) so converges to a limit. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 457--><p class="noindent" ><span class="head">
<a 
 id="x1-5002r2"></a>
<span 
class="cmbx-12">Proposition 5.2.</span>  </span> <span 
class="cmti-12">Every power series</span> \(\sum _0^\infty c_nz^n\)<span 
class="cmti-12">,</span> \(c_n \in \CC \) <span 
class="cmti-12">has a radius of convergence</span> \(R\) <span 
class="cmti-12">given by Hadamard&#x2019;s</span>
<span 
class="cmti-12">formula</span> \(\frac{1}{R} = \limsup |c_n|^{\frac{1}{n}}\)
</p>
  <div class="proof">
<!--l. 461--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>For any \(r&lt;R\) and large \(n\) we have \(|c_n|^{\frac{1}{n}}&lt; \frac{1}{r}\) so if \(|z|&lt;r&lt;R\) we have \(\sum _n^\infty |c_n|z^n &lt; \sum _n^\infty \frac{z^n}{r^n}\) so we have absolute convergence. Similarly
if \(|z|&gt;r&gt;R\) there are in&#xFB01;nitely many terms so that \(|c_n|\geq \frac{1}{r^n}\) so the series diverges. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 465--><p class="noindent" ><span class="head">
<a 
 id="x1-5003r3"></a>
<span 
class="cmbx-12">Proposition 5.3.</span>  </span><span 
class="cmti-12">Every power series</span> \(\sum _0^\infty c_nz^n\) <span 
class="cmti-12">is holomorphic in its radius of convergence, and its</span>
<span 
class="cmti-12">derivative,</span> \(\sum _0^\infty nc_nz^{n-1}\) <span 
class="cmti-12">has the same radius of convergence.</span>
</p>
  <div class="proof">
<!--l. 469--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>First  note  by  Proposition  <a 
href="#x1-5002r2">5.2<!--tex4ht:ref: radconv --></a>  since  \(n^{1/n} \to 1\)  as  \(n \to \infty \)  we  have  that  \(\sum _0^\infty nc_nz^{n-1}\)  has  the  same  radius  of
convergence.
</p><!--l. 471--><p class="indent" >  Let \(z_0,z_0+h\) lie in radius \(r\), which is less than the radius of convergence. \[\left |\frac{\sum _0^\infty c_j(z_0+h)^j-\sum _0^\infty c_jz_0^j}{h}-\sum _0^\infty c_jjz_0^{j-1}\right |\] \[= |\ee _n(h)| + \left |\frac{\sum _{n+1}^\infty c_j(z_0+h)^j-\sum _{n+1}^\infty c_jz_0^j}{h}\right |+|\sum _{n+1}^\infty c_jjz_0^{j-1}|\] where \(\ee _n(h)\) is the error from
the \(n^{th}\) partial sum&#x2019;s approximation by its derivative. Since \(a^j-b^j = (a-b)(a^{j-1}+a^{j-2}b+\dots +b^{j-1})\) we have \(|\frac{(z_0+h)^j-z_0^j}{h}|\leq jr^{j-1}\). For any \(\ee &gt;0\), we can choose
\(n\) large enough so that \(\sum _{n+1}^\infty jc_jr^{j-1}&lt;\ee \). Afterwards we can choose \(h\) small enough so \(\ee _n(h)&lt;\ee \). Then continuing from
above we get \[\leq \ee + \ee + \ee = 3\ee \] concluding the proof. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 480--><p class="noindent" ><span class="head">
<a 
 id="x1-5004r4"></a>
<span 
class="cmbx-12">Proposition 5.4 </span>&#x0028;Abel&#x2019;s Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If a power series</span> \(f(x) = \sum _0^\infty c_ix^i\) <span 
class="cmti-12">converges at an endpoint of its radius of</span>
<span 
class="cmti-12">convergence</span> \(R\)<span 
class="cmti-12">, it converges uniformly and hence is continuous on</span> \([0,R]\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 483--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>WLOG,\(R = 1\). Let \(A_n = \sum _0^{n-1}c_i\). We have then for \(x \in [0,1]\) \[ |\sum _n^mc_ix^i|=|\sum _n^mA_{i+1}x^i-\sum _n^mA_ix^i|=|\sum _n^m(f(1)-A_{i+1})x^i-\sum _n^m(f(1)-A_i)x^i|\]\[ = |\sum _{n+1}^{m+1}(f(1)-A_i)x^{i-1}-\sum _n^m(f(1)-A_i)x^i |\]\[= |\sum _{n+1}^{m}(f(1)-A_i)(x^{i-1}-x^i)-(f(1)-A_i)x^n+(f(1)-A_i)x^{m}| \] now choosing \(N\) large enough so \(|f(1)-A_i|&lt;\ee \) for \(i&gt;n\) and noting \(x \in [0,1]\) \[\leq \sum _{n+1}^{m}|(f(1)-A_i)|(x^{i-1}-x^i)+|(f(1)-A_i)|x^n+(f(1)-A_i)x^{m}|\]\[\leq (x^n-x^m)\ee +x^n\ee +x^m\ee \leq 3\ee \]
and we are done as the partial sums are uniformly Cauchy. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
                                                                                         
                                                                                         
  </div>
  <div class="newtheorem">
<!--l. 493--><p class="noindent" ><span class="head">
<a 
 id="x1-5005r5"></a>
<span 
class="cmbx-12">Proposition 5.5.</span>  </span><span 
class="cmti-12">If</span> \(f(x) = \sum _0^\infty c_iz^i\) <span 
class="cmti-12">converges within radius</span> \(R\)<span 
class="cmti-12">, and</span> \(a\) <span 
class="cmti-12">is within this radius,</span> \(f(a+x)\) <span 
class="cmti-12">converges and is</span>
<span 
class="cmti-12">analytic within radius of</span> \(R-|a|\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 497--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Inside \(|z|&lt;R-|a|\) the power series absolutely converges, and we would like to recenter around
0. WLOG \(a\) is real and non-negative. For \(0&lt;x&lt;R-|a|\) we have \[ \sum _0^\infty c_i(a+x)^i = \sum _0^\infty c_i \sum _{j=0}^i\binom i j a^{i-j}x^j \] We can then split the inner sum up into
separate terms, still absolutely convergent as everything is non-negative, and then we can
collect like powers of \(x\) to get \[\sum _0^\infty c_i(a+x)^i = \sum _0^\infty \Big (\sum _m^\infty c_i\binom i m a^{i-m}\Big )x^m\] so \(f(a+z)\) has a power series that converges on the interval \((0,R-|a|)\). Hence
its radius of convergence is at least \(R-|a|\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 507--><p class="noindent" ><span class="head">
<a 
 id="x1-5006r6"></a>
<span 
class="cmbx-12">Lemma 5.6.</span>  </span> <span 
class="cmti-12">The composite of two</span> \(\FF \)<span 
class="cmti-12">-analytic functions</span> \(\FF \to \FF \) <span 
class="cmti-12">is analytic.</span>
</p>
  <div class="proof">
<!--l. 510--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Real analytic functions are restrictions of complex analytic ones, and by the chain
rule for holomorphic functions, the composite is analytic. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 514--><p class="noindent" ><span class="head">
<a 
 id="x1-5007r7"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Lemma 5.7.</span>  </span> \((1+z)^a\) <span 
class="cmti-12">is holomorphic within radius</span> \(1\) <span 
class="cmti-12">around the origin, its power series given by</span> \(\sum _0^\infty \binom a j z^j\)<span 
class="cmti-12">.</span>
<span 
class="cmti-12">It converges absolutely and uniformly on the interval</span> \([-1,1]\) <span 
class="cmti-12">when</span> \(a&gt;0\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 518--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>That this is analytic follows from Lemma <a 
href="#x1-5006r6">5.6<!--tex4ht:ref: companalytic --></a>, and that it is holomorphic of radius
\(1\) comes from Hadamard&#x2019;s formula. Now when \(z \in [-1,1]\), for \(n&gt;a&gt;0\) we have \(n\bigg (1-\Big |\frac{\binom a{n+1}}{\binom a n}z\Big |\bigg ) \geq n\bigg (1-\Big |\frac{\binom a{n+1}}{\binom a n}\Big |\bigg ) = n\big (1-\frac{n-a}{n+1}\big ) = \frac{n(1+\alpha )}{n+1}\) which is larger than \(1\) for
su&#xFB03;ciently large \(n\) so by Lemma <a 
href="#x1-5001r1">5.1<!--tex4ht:ref: raabe --></a> and Theorem <a 
href="#x1-3017r17">3.17<!--tex4ht:ref: unifconv --></a> we have the last statement. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 522--><p class="noindent" ><span class="head">
<a 
 id="x1-5008r8"></a>
<span 
class="cmbx-12">Lemma 5.8.</span>  </span> <span 
class="cmti-12">The function</span> \(|x|\) <span 
class="cmti-12">can be uniformly approximated by polynomials in a bounded</span>
<span 
class="cmti-12">interval.</span>
</p>
  <div class="proof">
<!--l. 525--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>WLOG the interval is \([-1,1]\). By Lemma <a 
href="#x1-5007r7">5.7<!--tex4ht:ref: binom --></a> \((1-t)^{1/2}\) is uniformly approximated by its Taylor
expansion in this interval, and we can set \(t = 1-x^2\) we get \((x^2)^{1/2} = |x|\) is as well. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 529--><p class="noindent" ><span class="head">
<a 
 id="x1-5009r9"></a>
<span 
class="cmbx-12">Theorem 5.9 </span>&#x0028;Stone-Weierstrass&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(S\) <span 
class="cmti-12">is compact, any</span> \(\RR \)<span 
class="cmti-12">-subalgebra of</span> \(\cC (S,\RR )\) <span 
class="cmti-12">that separates points</span>
<span 
class="cmti-12">is dense.</span>
</p>
  <div class="proof">
<!--l. 533--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>Let \(A\) be a \(\RR \)-subalgebra of \(\cC (S,\RR )\) that separates points. If \(f \in \bar{A}\), then \(|f| \in \bar{A}\) as \(f\) has a bounded image so
by Lemma <a 
href="#x1-5008r8">5.8<!--tex4ht:ref: absapprox --></a> \(|f|\) can be approximated uniformly by polynomials in \(f\), which are in \(\bar{A}\).
</p><!--l. 535--><p class="indent" >  As a consequence, \(\max (f,g), \min (f,g) \in \bar{A}\) whenever \(f,g\) are as they are linear combinations of \(f,g,|f|,|g|\).
</p><!--l. 538--><p class="indent" >  Now for any \(\ee &gt;0,h \in \cC (S,\RR )\) we can approximate \(h\) up to \(\ee \) as follows: for each \(a \in S\) choose \(f_{a,b}\) for each \(b\) so that
\(f(a) = h(a),f(b) = h(b)\). Then in a small neighborhood of \(b\), \(f_{a,b}(x)&gt;h(x)-\ee \), but \(S\) is compact so &#xFB01;nitely many such neighborhoods
su&#xFB03;ce to have \(f_a = \max _{b_i}f_{a,b_i}\) be at least \(h(x)-\ee \) everywhere. Now we can similarly choose small neighborhoods
for each \(a\) so that \(f_a &lt; h(x)+\ee \), and once again &#xFB01;nitely many su&#xFB03;ce so that \(\min _{a_i}{f_{a_i}}\) is our desired approximation.
<span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 542--><p class="noindent" >There is an analogous version for \(\cC (S,\CC )\).
</p>
  <div class="newtheorem">
<!--l. 544--><p class="noindent" ><span class="head">
<a 
 id="x1-5010r10"></a>
<span 
class="cmbx-12">Corollary 5.10.</span>  </span><span 
class="cmti-12">If</span> \(S\) <span 
class="cmti-12">is compact, any</span> \(C^*\)<span 
class="cmti-12">-subalgebra of</span> \(\cC (S,\CC )\) <span 
class="cmti-12">that separates points is dense.</span>
</p>
  <div class="proof">
<!--l. 547--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Let \(A\) again be such a subalgebra. As \(A\) is closed under conjugation, it contains the real
and imaginary parts of any element \(f\). Then by Theorem <a 
href="#x1-5009r9">5.9<!--tex4ht:ref: stoweier --></a> the real and imaginary parts of
any function \(h\) are in \(\bar A\) so \(h\) is as well. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">6. </span> <a 
 id="x1-60006"></a>Endomorphisms</h3>
  <div class="newtheorem">
<!--l. 552--><p class="noindent" ><span class="head">
<a 
 id="x1-6001r1"></a>
<span 
class="cmbx-12">Lemma 6.1 </span>&#x0028;Adjugates&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">Let</span> \(M\) <span 
class="cmti-12">be a free</span> \(R\)<span 
class="cmti-12">-module. Given</span> \(f \in \End _R(M)\)<span 
class="cmti-12">, there is an element</span> \(\adj (f)\) <span 
class="cmti-12">such that</span>
\(f\adj (f) = \adj (f)f=1_M\)<span 
class="cmti-12">.</span>
</p>
                                                                                         
                                                                                         
  <div class="proof">
<!--l. 555--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>It su&#xFB03;ces to prove this in the universal ring \(R=\ZZ [x_{ij}]\), \(1\leq i,j\leq n\), with an endomorphism \(f\) given by
the matrix \((x_{ij})\). We have a natural isomorphism \(j:M \cong \Hom (\wedge ^{n-1}M,\wedge ^{n}M)\) so that \(j(x_1)(x_2\wedge \dots \wedge x_n) = x_1\wedge \dots \wedge x_n\). To get \(\adj f\), take the endomorphism
corresponding to \(\Hom (\wedge ^{n-1}f,1_{\wedge ^{n}M})\). Now we have \(j(\adj (f)f)\) takes \(x_2\wedge \dots \wedge x_n \mapsto f(x_2)\wedge \dots \wedge f(x_n) \mapsto f(x_1)\wedge \dots \wedge f(x_n)\) which is \(j(det(f)1_M)\). Note that since \(\det (f)\) is nonzero, \(\adj (f)\) has nonzero
determinant, so is injective as \(R\) is an integral domain. Now we have \(\adj (f)f\adj (f)=\adj (f)\det (f)\) so by injectivity \(f\adj (f) = \det (f)1_M\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 559--><p class="noindent" ><span class="head">
<a 
 id="x1-6002r2"></a>
<span 
class="cmbx-12">Lemma 6.2.</span>  </span> <span 
class="cmti-12">If</span> \(Av = \lambda v\) <span 
class="cmti-12">then for any polynomial</span> \(p\)<span 
class="cmti-12">,</span> \(p(A)v = p(\lambda )v\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 562--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>\(p(A)v=p(\lambda )v\) is a linear combination of \(A^nv=\lambda ^nv\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 566--><p class="noindent" ><span class="head">
<a 
 id="x1-6003r3"></a>
<span 
class="cmbx-12">Theorem 6.3 </span>&#x0028;Cayley-Hamilton&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">Let</span> \(M\) <span 
class="cmti-12">be a &#xFB01;nitely generated free</span> \(R\)<span 
class="cmti-12">-module, and</span> \(f \in \End (M)\)<span 
class="cmti-12">. Then</span>
\(\chi _f(f)\equiv 0\)
</p>
  <div class="proof">
<!--l. 569--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>\(f\) turns \(M\) into a \(R[T]\)-module, and we can extend scalars via \(R' = R[x]\otimes _R R[T]\) and \(M' = R[x]\otimes _R M\). Then \(\chi _f(x)\) is the determinant
of \(y = x\otimes 1_M - 1\otimes T \in R'\). \(\adj (y)\) commutes with \(y\) by Lemma <a 
href="#x1-6001r1">6.1<!--tex4ht:ref: adjugates --></a>, and since \(x\otimes 1_M\) is central it commutes with \(1\otimes T\) as well, but
then it commutes with all of \(R'\). Now we look at \(R'/(y),M'/(y)M'\) which substitutes \(x\) as \(T\), and note that \(\adj (y)\) has a
                                                                                         
                                                                                         
well-de&#xFB01;ned action on the quotient as it commutes with \(R'\). \(M'/(y)M'\cong 1\otimes M\) since \(g(x)\otimes m = (g(x)\otimes 1)(1\otimes m) = (1 \otimes g(T))(1\otimes m) = 1 \otimes g(T)m\). Then since \(y\) annihilates \(M'/(y)M'\), \(y\adj (y)\)
does as well, but this is multiplication by \(\chi _f(x)\otimes 1=1\otimes \chi _f(T)\), which is the action of \(\chi _f(f)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 573--><p class="noindent" ><span class="head">
<a 
 id="x1-6004r4"></a>
<span 
class="cmbx-12">Corollary 6.4 </span>&#x0028;Determinant Trick&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f\) <span 
class="cmti-12">is an endomorphism of</span> \(M\)<span 
class="cmti-12">, an</span> \(R\)<span 
class="cmti-12">-module generated by</span> \(n\)
<span 
class="cmti-12">elements, and</span> \(fM\subset IM\)<span 
class="cmti-12">,</span> \(f\) <span 
class="cmti-12">satis&#xFB01;es</span> \(f^n+a_{1}f^{n-1}+\dots +a_n \equiv 0\) <span 
class="cmti-12">where</span> \(a_i \in I^i\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 576--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By projectivity of free modules, it su&#xFB03;ces to consider a free module, but then this
follows from Theorem <a 
href="#x1-6003r3">6.3<!--tex4ht:ref: cayleyhamilton --></a> by noting that the coe&#xFB03;cients of \(\chi _f(x)\) are of the form described. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 580--><p class="noindent" ><span class="head">
<a 
 id="x1-6005r5"></a>
<span 
class="cmbx-12">Corollary 6.5 </span>&#x0028;Nakayama&#x2019;s Lemma&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(M\) <span 
class="cmti-12">is a &#xFB01;nitely generated</span> \(R\)<span 
class="cmti-12">-module and</span> \(IM=M\)<span 
class="cmti-12">, then there</span>
<span 
class="cmti-12">is an</span> \(a\equiv 1\pmod{I}\) <span 
class="cmti-12">such that</span> \(aM = 0\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 584--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Apply Corollary <a 
href="#x1-6004r4">6.4<!--tex4ht:ref: dettrick --></a> to the identity map \(1_M\) and use the fact that \(1_MM\subset IM\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 588--><p class="noindent" ><span class="head">
<a 
 id="x1-6006r6"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Corollary 6.6 </span>&#x0028;Nakayama&#x2019;s Lemma&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(M\) <span 
class="cmti-12">is a &#xFB01;nitely generated module over a local ring</span> \(R\)
<span 
class="cmti-12">with maximal ideal</span> \(m\) <span 
class="cmti-12">and</span> \(mM=M\)<span 
class="cmti-12">, then</span> \(M=0\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 592--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By Corollary <a 
href="#x1-6005r5">6.5<!--tex4ht:ref: nak1 --></a> \(aM=0\) for \(a\equiv 1\pmod{m}\) but then \(aM=M\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 596--><p class="noindent" ><span class="head">
<a 
 id="x1-6007r7"></a>
<span 
class="cmbx-12">Corollary 6.7 </span>&#x0028;Nakayama&#x2019;s Lemma&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(M\) <span 
class="cmti-12">is a &#xFB01;nitely generated module over a local ring</span> \(R\)
<span 
class="cmti-12">with maximal ideal</span> \(m\) <span 
class="cmti-12">and</span> \(R\) <span 
class="cmti-12">and</span> \(M = N+mM\) <span 
class="cmti-12">then</span> \(M=N\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 601--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Apply Corollary <a 
href="#x1-6006r6">6.6<!--tex4ht:ref: nak2 --></a> to \(M/N\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 605--><p class="noindent" ><span class="head">
<a 
 id="x1-6008r8"></a>
<span 
class="cmbx-12">Corollary 6.8 </span>&#x0028;Nakayama&#x2019;s Lemma&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(M\) <span 
class="cmti-12">is a &#xFB01;nitely generated module over a local ring</span> \(R\) <span 
class="cmti-12">with</span>
<span 
class="cmti-12">maximal ideal</span> \(m\) <span 
class="cmti-12">and the image of</span> \(m_1,\dots ,m_n\) <span 
class="cmti-12">generate</span> \(M/mM\)<span 
class="cmti-12">, then</span> \(m_1,\dots ,m_n\) <span 
class="cmti-12">generate</span> \(M\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 608--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Apply Corollary <a 
href="#x1-6007r7">6.7<!--tex4ht:ref: nak3 --></a> with \(N = \sum _1^nm_iM\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
<!--l. 612--><p class="noindent" >Note if the ring is not local, we can replace \(m\) by the Jacobson radical and Nakayama&#x2019;s Lemma still
holds.
</p>
  <div class="newtheorem">
<!--l. 614--><p class="noindent" ><span class="head">
<a 
 id="x1-6009r9"></a>
<span 
class="cmbx-12">Proposition 6.9.</span>  </span><span 
class="cmti-12">Every endomorphism</span> \(f:V\to V\) <span 
class="cmti-12">on a &#xFB01;nite dimensional vector space</span> \(V\) <span 
class="cmti-12">over</span> \(F\) <span 
class="cmti-12">has a</span>
<span 
class="cmti-12">minimal polynomial</span> \(\mu _f\)<span 
class="cmti-12">, satisfying</span> \(\mu _f(f)=0\)<span 
class="cmti-12">,</span> \(g(f)=0 \implies \mu _f|g\)<span 
class="cmti-12">, and its roots are the eigenvalues.</span>
</p>
  <div class="proof">
<!--l. 618--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Viewing \(V\) as a \(F[T]\) module, since \(F[T]\) is a PID, everything is immediate except the last part,
which follows since \(f(v)=\lambda v\) so by Lemma <a 
href="#x1-6002r2">6.2<!--tex4ht:ref: polysub --></a> \(\mu _f(f)(v)=\mu _f(\lambda )v\) but the LHS is \(0\) and \(v\) is not so we are done. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 622--><p class="noindent" ><span class="head">
<a 
 id="x1-6010r10"></a>
<span 
class="cmbx-12">Proposition 6.10.</span>  </span> <span 
class="cmti-12">Submodules</span> \(M\) <span 
class="cmti-12">of</span> \(R^n\)<span 
class="cmti-12">, a &#xFB01;nite generated free module over a PID are after a</span>
<span 
class="cmti-12">change of basis of the form</span> \(\bigoplus _1^n x_ir_iR\) <span 
class="cmti-12">with</span> \(x_i|x_{i+1} \in R\) <span 
class="cmti-12">and</span> \(\bigoplus _1^n r_iR = R^n\) <span 
class="cmti-12">&#x0028;the</span> \(r_i\) <span 
class="cmti-12">are the change of basis&#x0029;. This representation is</span>
<span 
class="cmti-12">unique up to units and the</span> \(x_i\) <span 
class="cmti-12">are called the </span><span 
class="cmbxti-10x-x-120">invariant factors</span><span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 626--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Choose a map \(f_1:R^n\to R\) where the image of \(M\) is maximized &#x0028;this uses PID&#x0029;. Let \(y_1\) be an element
sent to a generator of the image, \(x_1\), which WLOG is nonzero. Now if \(\pi _i\) is the \(i^{th}\) projection, then \(x_1|\pi _i(y_1)\)
for all \(x \in R^n\) by maximality of \(f_1\), so we can let \(r_1 = \frac{y_1}{x_1}\). Now \(r_1\) gets sent to \(1\) by \(f\), so we can project orthogonal
to \(r_1\) via a section \(s_1:R \to R^n\) taking \(1 \mapsto r_1\). Our projection \(o_1(x) = x-s_1\circ f_1(x)\). This section gives \(R^n = r_1R\oplus o_1(R)\). The projection to \(o_1(R)\) is surjective,
and by removing an appropriate generator and localizing at \((0)\), we see that our new module
must be free of rank \(n-1\). Now we apply induction to get \(y_2,\dots ,y_n\) and \(r_2,\dots ,r_n\), and by looking at \(f_1\) we get \(x_1|x_2\).
Uniqueness also follows from induction. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 630--><p class="noindent" ><span class="head">
<a 
 id="x1-6011r11"></a>
<span 
class="cmbx-12">Corollary 6.11 </span>&#x0028;Smith Canonical Form&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">A map</span> \(f:M \to N\) <span 
class="cmti-12">between &#xFB01;nitely generated free modules over a</span>
<span 
class="cmti-12">PID of ranks</span> \(n\) <span 
class="cmti-12">and</span> \(m\) <span 
class="cmti-12">has a Smith Canonical Form, ie. is represented by a matrix of the form </span>\[\begin{pmatrix} x_1 &amp; &amp;\\ &amp; \ddots &amp; \\ &amp; &amp; x_{\min{(n,m)}} \end{pmatrix}\] <span 
class="cmti-12">with</span> \(x_i|x_{i+1}\)<span 
class="cmti-12">.</span>
<span 
class="cmti-12">This representation is unique up to units.</span>
</p>
  <div class="proof">
<!--l. 639--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>The image is a submodule of \(R^m\) so this is a reformulation of Proposition <a 
href="#x1-6010r10">6.10<!--tex4ht:ref: fgfreePID --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 643--><p class="noindent" ><span class="head">
<a 
 id="x1-6012r12"></a>
<span 
class="cmbx-12">Corollary 6.12 </span>&#x0028;Finitely Generated Modules over PIDs&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">A &#xFB01;nitely generated module over</span>
<span 
class="cmti-12">a PID is of the form</span> \(R^m \oplus \bigoplus _1^nR/(d_i)\) <span 
class="cmti-12">where</span> \(d_i|d_{i+1}\)<span 
class="cmti-12">. Moreover</span> \(m\) <span 
class="cmti-12">is unique, and the</span> \(d_i\) <span 
class="cmti-12">are unique up to units.</span>
</p>
  <div class="proof">
<!--l. 647--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>A &#xFB01;nitely generated module over a PID is a quotient of a &#xFB01;nite rank free module,
which has the correct form according to Proposition <a 
href="#x1-6010r10">6.10<!--tex4ht:ref: fgfreePID --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 652--><p class="noindent" ><span class="head">
<a 
 id="x1-6013r13"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Corollary 6.13 </span>&#x0028;Rational Canonical Form&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">Every endomorphism</span> \(f:V\to V\) <span 
class="cmti-12">of a &#xFB01;nite dimensional vector</span>
<span 
class="cmti-12">space over</span> \(F\) <span 
class="cmti-12">has a unique Rational Canonical Form, ie. is represented by a matrix of the form </span>\[\bigoplus _{i=1}^n\begin{pmatrix} 0&amp;0&amp;0&amp;\dots&amp; -a_0\\ 1&amp;0&amp;0&amp;\dots&amp; -a_1\\ 0&amp;1&amp;0&amp;\dots&amp; -a_2\\ \vdots&amp; \vdots&amp; \vdots&amp; \ddots&amp; \vdots \\ 0&amp;0&amp;0&amp;\dots&amp; -a_{k_i-1} \end{pmatrix} \]
<span 
class="cmti-12">where the monic polynomials &#x0028;invariant factors&#x0029;</span> \(f_i(x)=\sum _1^{k_i}a_ix^i\) <span 
class="cmti-12">satisfy</span> \(f_i|f_{i+1}\)<span 
class="cmti-12">.</span> \(f_n\) <span 
class="cmti-12">is</span> \(\mu _f(x)\) <span 
class="cmti-12">and</span> \(\prod _if_i\) <span 
class="cmti-12">is</span> \(\chi _f(x)\)
</p>
  <div class="proof">
<!--l. 664--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>View \(V\) as a module over \(F[T]\), and we get \(V \cong \sum _1^nF[T]/(f_i)\) from Corollary <a 
href="#x1-6012r12">6.12<!--tex4ht:ref: fgmodPID --></a>. Then we are done by
picking \(1,T,\dots ,T^{k_i-1}\) as a basis. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 667--><p class="noindent" ><span class="head">
<a 
 id="x1-6014r14"></a>
<span 
class="cmbx-12">Corollary 6.14.</span>  </span><span 
class="cmti-12">If</span> \(A\) <span 
class="cmti-12">a matrix over</span> \(F\)<span 
class="cmti-12">, the invariant factors can be computed by &#xFB01;nding the Smith</span>
<span 
class="cmti-12">Canonical Form of</span> \(xI-A\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 670--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If \(V\) is dimension \(n\) we can consider the \(F[T]\) module homomorphism \(F[T]^n \to V\) mapping the generators
\(r_i\) surjectively onto an \(F\)-basis \(v_i\) of \(V\). Now the elements \(y_i=Tr_j-\sum _1^i(a_{ij}r_i)\) are in the kernel but note that \(\sum _iy_iF[T] + \sum _ir_iF = \sum _ir_iF[T] = F[T]^n\), so \(y_i\)
actually generate the kernel. The \(y_i\) have the relations matrix \(xI-A^{t}\), so by Corollary <a 
href="#x1-6011r11">6.11<!--tex4ht:ref: smithcanform --></a> after a
change of basis it is in Smith Normal Form with invariant factors \(f_1,\dots ,f_n\), so the kernel is of this
form for an appropriate set of generators, and \(V \cong \bigoplus _1^nF[T]/(f_n)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 674--><p class="indent" >  Note that Corollary <a 
href="#x1-6012r12">6.12<!--tex4ht:ref: fgmodPID --></a> also can be represented as \(R^m \oplus \bigoplus R/(p^i)\) where \(p\) varies over primes, and similarly
Corollary <a 
href="#x1-6013r13">6.13<!--tex4ht:ref: ratcanform --></a> has a representation in this way.
</p>
  <div class="newtheorem">
<!--l. 675--><p class="noindent" ><span class="head">
<a 
 id="x1-6015r15"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Corollary 6.15 </span>&#x0028;Jordan Canonical Form&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">Every endomorphism</span> \(f:V\to V\) <span 
class="cmti-12">of a &#xFB01;nite dimensional vector</span>
<span 
class="cmti-12">space over</span> \(F\) <span 
class="cmti-12">has a unique Jordan Canonical Form after extending scalars to an algebraic</span>
<span 
class="cmti-12">closure, ie. is represented by a matrix of the form </span>\[\bigoplus _{i=1}^n \begin{pmatrix} \lambda _i&amp;1&amp;0&amp;\dots&amp; 0\\ 0&amp;\lambda _i&amp;1&amp;\dots&amp; 0\\ 0&amp;0&amp;\lambda _i&amp;\dots&amp; 0\\ \vdots&amp; \vdots&amp; \vdots&amp; \ddots&amp; \vdots \\ 0&amp;0&amp;0&amp;\dots&amp; \lambda _i \end{pmatrix} \] <span 
class="cmti-12">Each summand is called a </span><span 
class="cmbxti-10x-x-120">Jordan</span>
<span 
class="cmbxti-10x-x-120">block</span><span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 688--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>As an \(F[T]\)-module, decompose \(V \cong \bigoplus _{i=1}^n F[T]/(T-\lambda _i)^j_i\), and choose as a basis for each summand \(1,T-\lambda _i,\dots ,(T-\lambda _i)^{j_i-1}\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 692--><p class="noindent" ><span class="head">
<a 
 id="x1-6016r16"></a>
<span 
class="cmbx-12">Corollary 6.16 </span>&#x0028;Diagonalization Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">A  matrix  is  diagonalizable  i&#xFB00;  its  Jordan</span>
<span 
class="cmti-12">Canonical Form is diagonal i&#xFB00; its minimal polynomial is separable.</span>
</p>
  <div class="proof">
<!--l. 696--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By uniqueness of the Jordan Canonical Form the &#xFB01;rst statement is true, and since
the minimal polynomial is the LCM of the minimal polynomials of the Jordan blocks, so it
must have distinct roots. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 700--><p class="noindent" ><span class="head">
<a 
 id="x1-6017r17"></a>
<span 
class="cmbx-12">Proposition 6.17.</span>  </span><span 
class="cmti-12">Commuting   diagonalizable   endomorphisms</span>   \(A,B\)   <span 
class="cmti-12">are   simultaneously</span>
<span 
class="cmti-12">diagonalizable.</span>
</p>
  <div class="proof">
<!--l. 704--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>If \(v\) has eigenvalue \(\lambda \) for \(A\), then \(BAv = ABv = \lambda Av\), so \(B\)&#x2019;s \(\lambda \)-eigenspace is \(A\)-invariant, so we can simultaneously
diagonalize. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">7. </span> <a 
 id="x1-70007"></a>Bilinear Maps</h3>
  <div class="newtheorem">
<!--l. 709--><p class="noindent" ><span class="head">
<a 
 id="x1-7001r1"></a>
<span 
class="cmbx-12">Lemma 7.1.</span>  </span> <span 
class="cmti-12">If</span> \(f:\Sym ^2V\to F\) <span 
class="cmti-12">is a map of</span> \(F\)<span 
class="cmti-12">-vector spaces which is nontrivial, and</span> \(\ch (F)\neq 2\)<span 
class="cmti-12">, there is an element</span>
<span 
class="cmti-12">so that</span> \(f(v\otimes v)\neq 0\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 713--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>There is some \(v_1,v_2\) such that \(f(v_1\otimes v_2)\neq 0\). Now WLOG, \(v_1\neq v_2\) \(f(v_1\otimes v_1)=f(v_2\otimes v_2)=0\), so we have \(\sum _{1\leq i,j\leq 2}f(v_i\otimes v_j) = f((v_1+v_2)\otimes (v_1+v_2))\neq 0\) <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 717--><p class="noindent" ><span class="head">
<a 
 id="x1-7002r2"></a>
<span 
class="cmbx-12">Proposition 7.2 </span>&#x0028;Decomposition Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span>  <span 
class="cmti-12">If</span>  \(f:\Sym ^2V\to F\)  <span 
class="cmti-12">is  a  map  of</span>  \(F\)<span 
class="cmti-12">-vector  spaces  which  is</span>
<span 
class="cmti-12">nontrivial, for any element</span> \(v \in V\) <span 
class="cmti-12">so that</span> \(f(v\otimes v)\neq 0\)<span 
class="cmti-12">,</span> \(V \cong V'\oplus vF\)<span 
class="cmti-12">, where</span> \(V'\) <span 
class="cmti-12">consists of vectors</span> \(f\)<span 
class="cmti-12">-orthogonal to</span> \(v\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 721--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>\(\pi (x) = x-v\frac{f(v\otimes x)}{f(v\otimes v)}\) is a projection onto \(V'\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
                                                                                         
                                                                                         
  </div>
  <div class="newtheorem">
<!--l. 725--><p class="noindent" ><span class="head">
<a 
 id="x1-7003r3"></a>
<span 
class="cmbx-12">Corollary 7.3 </span>&#x0028;Graham-Schmidt Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span>  <span 
class="cmti-12">If</span>  \(f:\Sym ^2V\to F\)  <span 
class="cmti-12">is  a  map  of</span>  \(F\)<span 
class="cmti-12">-vector  spaces,</span>  \(V\)  <span 
class="cmti-12">is  &#xFB01;nite</span>
<span 
class="cmti-12">dimensional, and</span> \(\ch (F)\neq 2\)<span 
class="cmti-12">, then</span> \(V\) <span 
class="cmti-12">has an</span> \(f\)<span 
class="cmti-12">-orthogonal basis. Hence</span> \(f\) <span 
class="cmti-12">is represented by a diagonal matrix.</span>
</p>
  <div class="proof">
<!--l. 729--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If \(f\) is \(0\), any basis will do. If not, by induction, Lemma <a 
href="#x1-7001r1">7.1<!--tex4ht:ref: char2lemma --></a> and Proposition <a 
href="#x1-7002r2">7.2<!--tex4ht:ref: vectdecomp --></a> we are
done. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 733--><p class="noindent" ><span class="head">
<a 
 id="x1-7004r4"></a>
<span 
class="cmbx-12">Corollary 7.4 </span>&#x0028;Sylvester&#x2019;s Law of Inertia&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f:\Sym ^2V\to \RR \) <span 
class="cmti-12">is a map of</span> \(\RR \)<span 
class="cmti-12">-vector spaces,</span> \(V\) <span 
class="cmti-12">is &#xFB01;nite dimensional,</span>
<span 
class="cmti-12">then</span> \(f\) <span 
class="cmti-12">is represented by a unique matrix of the form</span> \(I_n\oplus -I_m\oplus 0_r\)<span 
class="cmti-12">. &#x0028;Congruent real symmetric matrices have the</span>
<span 
class="cmti-12">same rank and signature&#x0029;.</span>
</p>
  <div class="proof">
<!--l. 736--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Once we have diagonalized a matrix representing \(f\) from Corollary <a 
href="#x1-7003r3">7.3<!--tex4ht:ref: grahamschmidt --></a>, we can scale
each diagonal by a square, giving \(1,-1,0\). This form is unique as \(V\) decomposes into \(V_+\),\(V_-\),\(V_0\) where \(V_+\) is
the span of vectors \(v\) with \(f(v\otimes v)=1\) and similarly for the other two. By orthogonality, \(f\) is positive
de&#xFB01;nite in \(V_+\) and negative de&#xFB01;nite in \(V_-\), so these subspaces, and hence the rank and signature,
are invariant. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 740--><p class="noindent" ><span class="head">
<a 
 id="x1-7005r5"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Proposition 7.5 </span>&#x0028;Sylvester&#x2019;s Criterion&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">A symmetric real matrix</span> \(A\) <span 
class="cmti-12">is positive de&#xFB01;nite i&#xFB00; the</span>
<span 
class="cmti-12">principle minors are positive, and negative i&#xFB00; the odd principle minors are negative and the</span>
<span 
class="cmti-12">even ones positive.</span>
</p>
  <div class="proof">
<!--l. 744--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If \(A\) is positive de&#xFB01;nite, it is of the form \(P^tP\) for invertible \(P\), so \(\det (A)=\det (P)^2&gt;0\). Every principle submatrix
is  positive  de&#xFB01;nite  on  the  corresponding  subspace  so  the  principle  minors  are  positive.
Conversely if \(A\) has positive principle minors, we induct. Write \(A = BA'B^t\), \[A = \begin{pmatrix} A_0&amp;a\\ a^t&amp;\alpha \end{pmatrix}, B = \begin{pmatrix} I_{n-1}&amp;0\\ a^tA_0^{-1}&amp;1 \end{pmatrix}, A' = \begin{pmatrix} A_0&amp;0\\ 0&amp;\alpha -a^tA_0a \end{pmatrix}\] So \(\alpha -a^tA_0a\) is positive by the fact
that the determinant is positive and \(A_0\) is positive de&#xFB01;nite by induction, so we are done. For
the negative de&#xFB01;nite case note \(A\) is negative de&#xFB01;nite i&#xFB00; \(-A\) is positive de&#xFB01;nite. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 759--><p class="noindent" ><span class="head">
<a 
 id="x1-7006r6"></a>
<span 
class="cmbx-12">Corollary 7.6 </span>&#x0028;Hessian Test&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">A critical point</span> \(c\) <span 
class="cmti-12">of a</span> \(\cC ^2\) <span 
class="cmti-12">function</span> \(f:\RR ^n\to \RR \) <span 
class="cmti-12">is a local maximum i&#xFB00; the</span>
<span 
class="cmti-12">Hessian matrix</span> \((\partial _i\partial _jf(c))\) <span 
class="cmti-12">is negative de&#xFB01;nite, and a local minimum i&#xFB00; the Hessian matrix is positive</span>
<span 
class="cmti-12">de&#xFB01;nite.</span>
</p>
  <div class="proof">
<!--l. 763--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If  this  matrix  is  positive  de&#xFB01;nite,  then  by  the  Taylor  expansion,  \(f(x-c)-f(c)\)  is  closely
approximated  by  positive  de&#xFB01;nite  quadratic  function,  so  is  larger  than  \(0\)  in  a  deleted
neighborhood of \(0\). The corresponding argument yields the other result. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">8. </span> <a 
 id="x1-80008"></a>Inner Product Spaces</h3>
  <div class="newtheorem">
<!--l. 769--><p class="noindent" ><span class="head">
<a 
 id="x1-8001r1"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Proposition 8.1.</span>  </span><span 
class="cmti-12">Every endomorphism</span> \(T:V\to V\) <span 
class="cmti-12">on a &#xFB01;nite dimensional inner product space</span> \(V\) <span 
class="cmti-12">has a</span>
<span 
class="cmti-12">unique adjoint</span> \(T^*\) <span 
class="cmti-12">so that</span> \((Tu,v)=(u,T^*v)\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 773--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Choose an orthonormal basis, yielding an isometry with the standard inner product
space on \(\CC ^n\). Now if \(T\) is represented by the matrix \(A\), \(T^*\) is represented by \(\bar{A}^t\) as we have \((Au,v) = (Au)^t\cdot \bar{v} = u^t\overline{\bar{A}^tv} = (u,\bar{A}^tv)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 777--><p class="noindent" ><span class="head">
<a 
 id="x1-8002r2"></a>
<span 
class="cmbx-12">Proposition 8.2.</span>  </span> <span 
class="cmti-12">If</span> \(V\) <span 
class="cmti-12">is a &#xFB01;nite dimensional inner product space,</span> \(T\) <span 
class="cmti-12">a linear operator,</span> \(\ker (T) = T^*V^\perp \)<span 
class="cmti-12">,</span> \((W^\perp )^\perp = W, W \subset V\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 781--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>For the &#xFB01;rst part, \(v \in \ker (T)\) i&#xFB00; \(0 = (Tv,u) = (v,T^*u)\) for all \(u\), and the second part follows from the fact that we can
have an orthonormal basis for \(W\), and extend it to an orthonormal basis for \(V\), the remaining
vectors making up a basis for \(W^\perp \). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 785--><p class="noindent" ><span class="head">
<a 
 id="x1-8003r3"></a>
<span 
class="cmbx-12">De&#xFB01;nition 8.3.</span>  </span><span 
class="cmti-12">An  endomorphism  is  </span><span 
class="cmbxti-10x-x-120">normal </span><span 
class="cmti-12">if  it  commutes  with  its  adjoint,  and  is</span>
<span 
class="cmbxti-10x-x-120">self-adjoint </span><span 
class="cmti-12">if it is its own adjoint.</span>
</p>
  </div>
  <div class="newtheorem">
<!--l. 788--><p class="noindent" ><span class="head">
<a 
 id="x1-8004r4"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Theorem 8.4.</span>  </span> <span 
class="cmti-12">A normal endomorphism</span> \(T\) <span 
class="cmti-12">on a &#xFB01;nite dimensional vector space</span> \(V\) <span 
class="cmti-12">satis&#xFB01;es</span> \(\Vert Tv\Vert = \Vert T^*v\Vert \)<span 
class="cmti-12">,</span> \(\ker T = \ker T^*\)<span 
class="cmti-12">,</span> \(TV = T^*V\)<span 
class="cmti-12">,</span> \(T-\lambda 1_V\) <span 
class="cmti-12">is</span>
<span 
class="cmti-12">normal,</span> \(Tv = \lambda v \leftrightarrow T^*v = \bar{\lambda }v\)<span 
class="cmti-12">, and if</span> \(u,v\) <span 
class="cmti-12">are eigenvectors of</span> \(T\) <span 
class="cmti-12">with di&#xFB00;erent eigenvalues, then</span> \((u,v) =0\)
</p>
  <div class="proof">
<!--l. 791--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>For the &#xFB01;rst, we have \((Tv,Tv) = (v,T^*Tv)=(v,(T^*)^*T^*v) = (T^*v,T^*v)\). The second immediately follows, and the third follows from
the second and Proposition <a 
href="#x1-8002r2">8.2<!--tex4ht:ref: orthcomp --></a>. For the fourth we have \((T-\lambda 1_V)(T-\lambda 1_V)^* = T^*T-\bar{\lambda }T-\lambda T^*+|\lambda |^21_V = (T-\lambda 1_V)^*(T-\lambda 1_V)\). The &#xFB01;fth follows from the fourth
and the second, and the last follows from the fact that \(\lambda _1(u,v) = (Tu,v) = (u,T^*v) = \lambda _2(u,v)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 795--><p class="noindent" ><span class="head">
<a 
 id="x1-8005r5"></a>
<span 
class="cmbx-12">Proposition 8.5.</span>  </span><span 
class="cmti-12">If</span> \(T\) <span 
class="cmti-12">is an endomorphism on a &#xFB01;nite dimensional vector space</span> \(V\) <span 
class="cmti-12">and</span> \(\Vert Tv\Vert = \Vert T^*v\Vert \) <span 
class="cmti-12">then</span> \(T\)
<span 
class="cmti-12">is normal.</span>
</p>
  <div class="proof">
<!--l. 799--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>\[(T(u+v),T(u+v)) = (T^*(u+v),T^*(u+v))\implies (Tu,Tv) \]\[= (T^*u,T^*v)\implies (u,TT^*v) = (u,T^*Tv)\]. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 803--><p class="noindent" ><span class="head">
<a 
 id="x1-8006r6"></a>
<span 
class="cmbx-12">Theorem 8.6 </span>&#x0028;Spectral Theorem for \(\CC \)&#x0029;<span 
class="cmbx-12">.</span>  </span>  <span 
class="cmti-12">An  endomorphism</span>  \(T\)  <span 
class="cmti-12">of  an  &#xFB01;nite  dimensional</span>
<span 
class="cmti-12">complex inner product space</span> \(V\) <span 
class="cmti-12">is normal i&#xFB00; there is an orthonormal basis of eigenvectors, ie.</span>
<span 
class="cmti-12">there is a diagonal matrix representing it.</span>
</p>
  <div class="proof">
<!--l. 807--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>If there is an orthonormal basis of eigenvectors \(v_1,\dots v_n\), \(T\) with respect to this basis is diagonal,
so commutes with its adjoint. Conversely if \(T\) is normal, its minimal polynomial has a root so
it has a nontrivial eigenvector \(v\). Then by Theorem <a 
href="#x1-8004r4">8.4<!--tex4ht:ref: normalendfacts --></a>, if \(u\) is in the orthogonal complement,
\((Tu,v) = (u,T^*v)= \bar{\lambda }(u,v) = 0\), so \(T\) acts within the orthogonal complement and we can repeat until we get an orthonormal
basis of eigenvectors. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 812--><p class="noindent" ><span class="head">
<a 
 id="x1-8007r7"></a>
<span 
class="cmbx-12">Lemma 8.7.</span>  </span><span 
class="cmti-12">The eigenvalues of a self-adjoint endomorphism</span> \(T\) <span 
class="cmti-12">on a vector space</span> \(V\) <span 
class="cmti-12">are real.</span>
</p>
  <div class="proof">
<!--l. 816--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>This follows from the &#xFB01;fth part of Theorem <a 
href="#x1-8004r4">8.4<!--tex4ht:ref: normalendfacts --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 820--><p class="noindent" ><span class="head">
<a 
 id="x1-8008r8"></a>
<span 
class="cmbx-12">Theorem 8.8 </span>&#x0028;Spectral Theorem for \(\RR \)&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">An endomorphism</span> \(T\) <span 
class="cmti-12">of a &#xFB01;nite dimensional real</span>
<span 
class="cmti-12">inner product space</span> \(V\) <span 
class="cmti-12">is self-adjoint i&#xFB00; there is an orthonormal basis of eigenvectors, ie. there</span>
<span 
class="cmti-12">is a diagonal matrix representing it.</span>
</p>
  <div class="proof">
<!--l. 824--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If there is an orthonormal basis of eigenvectors, \(T\) is represented by a diagonal matrix,
which is self-adjoint. Conversely if \(T\) is self-adjoint, we can tensor with \(\CC \) and &#xFB01;nd an eigenvector,
but its eigenvalue is real, and \(T\) is real, so its conjugate is an eigenvector as well, but then
either their sum is a real eigenvector or it is \(0\) in which case we can multiply our original
eigenvector by \(i\) to get a real eigenvector. In either case, there must be a real eigenvector, so
                                                                                         
                                                                                         
by Theorem <a 
href="#x1-8004r4">8.4<!--tex4ht:ref: normalendfacts --></a> \(T\) acts within the orthogonal complement again, so we can repeat until we
get an orthonormal basis of eigenvectors. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">9. </span> <a 
 id="x1-90009"></a>Geometry of Mappings</h3>
  <div class="newtheorem">
<!--l. 830--><p class="noindent" ><span class="head">
<a 
 id="x1-9001r1"></a>
<span 
class="cmbx-12">Theorem 9.1 </span>&#x0028;Inverse Mapping Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f:\FF ^n\to \FF ^n\) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">near a point</span> \(a\) <span 
class="cmti-12">and</span> \(f'(a)\) <span 
class="cmti-12">is invertible near the</span>
<span 
class="cmti-12">origin, then</span> \(f\) <span 
class="cmti-12">is a</span> \(\cC ^1\) <span 
class="cmti-12">di&#xFB00;eomorphism near</span> \(a\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 834--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>WLOG, \(a=f(a) = 0, f'(a) = I\) via an a&#xFB03;ne transformation. Now as \(r(x) = f(x)-x\) is \(\cC ^1\) near the origin and \(r'(0) = 0\), we have \(\Vert r'(x) \Vert _o\leq 1/2\) near
the origin. Then by Corollary <a 
href="#x1-4007r7">4.7<!--tex4ht:ref: mvineq --></a> we have \(\Vert r(b)-r(a)\Vert _2\leq \frac{1}{2}\Vert b-a\Vert _2\) near the origin which gives \[\Vert f(b)-f(a)\Vert = \Vert f(b)-f(a)\Vert +\frac{1}{2}\Vert b-a\Vert -\frac{1} 2 \Vert b-a\Vert \]\[ \geq \Vert f(b)-f(a)\Vert + \Vert r(b)-r(a)\Vert -\frac{1}{2} \Vert b-a\Vert \geq \frac{1}{2}\Vert b-a\Vert \]
</p><!--l. 838--><p class="indent" >  Hence the map is injective, and by Theorem <a 
href="#x1-3013r13">3.13<!--tex4ht:ref: invardomain --></a>, this is a local homeomorphism near the
origin. Now let \(g\) be the local inverse of \(f\) near \(0\), we would like to show \(g\) is di&#xFB00;erentiable at \(0\). If \(f(x)=y\)
and \(f(x+h) = y+k\), then as \(f\) is di&#xFB00;erentiable, \[ f(x+h) - f(x) = f'(x)h + \ee _f(h) \implies k = f'(x)(g(y+k)-g(y)) + \ee _f(h) \] \[\implies g(y+k)-g(y) = f'(x)^{-1}k-f'(x)^{-1}\ee _f(h)\] and as \(\Vert f'(x)^{-1}\Vert _o\) is bounded near \(0\), it su&#xFB03;ces to show \(\frac{\Vert \ee _f(h)\Vert }{\Vert k\Vert } \to 0\) as \(k \to 0\).
</p><!--l. 844--><p class="indent" >  Indeed, we have \[\frac{\Vert \ee _f(h)\Vert }{\Vert k\Vert }=\frac{\Vert \ee _f(h)\Vert }{\Vert h\Vert }\frac{\Vert h\Vert }{\Vert k \Vert } = \frac{\Vert \ee _f(h)\Vert }{\Vert h\Vert }\frac{\Vert h\Vert }{\Vert f(x+h)-f(x) \Vert }\leq 2\frac{\Vert \ee _f(h)\Vert }{\Vert h\Vert }\frac{\Vert h\Vert }{\Vert h \Vert } \] which goes to \(0\) as \(k \to 0\). Thus \(g\) is di&#xFB00;erentiable and since its derivative is the
inverse of \(f'\), \(g\) is \(\cC ^1\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 850--><p class="noindent" ><span class="head">
<a 
 id="x1-9002r2"></a>
<span 
class="cmbx-12">Theorem 9.2 </span>&#x0028;Decomposition Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If a map</span> \(f:\RR ^n \to \RR ^n\) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">near a point</span> \(a\) <span 
class="cmti-12">and</span> \(\det (f'(a)) \neq 0\)<span 
class="cmti-12">, then near</span> \(a\)<span 
class="cmti-12">,</span> \(f\) <span 
class="cmti-12">is</span>
<span 
class="cmti-12">the composite of</span> \(\cC ^1\) <span 
class="cmti-12">di&#xFB00;eomorphisms</span> \(\phi _i\) <span 
class="cmti-12">that only change one variable.</span>
</p>
  <div class="proof">
<!--l. 854--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>WLOG, \(a = f(a) = 0\). We say that \(f\) is of type \(r\) if \(f\) doesn&#x2019;t change at least \(r-1\) coordinates. By induction
it su&#xFB03;ces to show that if \(f\) is type \(r\), then there is a \(\cC ^1\) di&#xFB00;eomorphism \(\phi \) so that \(f \circ \phi \) is type \(r+1\), so
we assume \(f\) &#xFB01;xes the &#xFB01;rst \(r-1\) coordinates, denoting these \(x_I\), and denoting the last \(n-r\) coordinates \(x_{II}\).
Then after some relabeling \(f'\) looks like \[\begin{pmatrix} I_{r-1}&amp;0&amp;0\\ \partial _If_r&amp;\partial _rf_r&amp;\partial _{II}f_r\\ \partial _If_{II}&amp;\partial _rf_{II}&amp;\partial _{II}f_{II} \end{pmatrix}\] Since \(f'(x)\) is invertible near 0, we can assume \(\partial _rf_r \neq 0\) near by
relabeling the \(f_i\). Then we can de&#xFB01;ne \(\psi \) near \(0\) as the function that is \(f_r\) on the \(x_r\) coordinate and the
identity on all other coordinates. Its derivative looks like \[\begin{pmatrix} I_{r-1}&amp;0&amp;0\\ \partial _If_r&amp;\partial _rf_r&amp;\partial _{II}f_r\\ 0&amp;0&amp;I_{n-r} \end{pmatrix}\] so is invertible and by Theorem
<a 
href="#x1-9001r1">9.1<!--tex4ht:ref: invmapping --></a> has a local inverse \(\phi \), which is what we want. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 870--><p class="noindent" >Note that the \(m =n\) case of the next theorem is the Inverse Mapping Theorem.
</p>
  <div class="newtheorem">
<!--l. 871--><p class="noindent" ><span class="head">
<a 
 id="x1-9003r3"></a>
<span 
class="cmbx-12">Theorem 9.3.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR ^m\to \RR ^n, m\geq n\) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">near a point</span> \(a\) <span 
class="cmti-12">with</span> \(f'(a)\) <span 
class="cmti-12">rank</span> \(n\)<span 
class="cmti-12">, then there is a</span> \(\cC ^1\) <span 
class="cmti-12">di&#xFB00;eomorphism</span> \(\phi :\RR ^m \to \RR ^m\) <span 
class="cmti-12">near</span> \(a\) <span 
class="cmti-12">so</span>
<span 
class="cmti-12">that</span> \(f\circ \phi -f(a)\) <span 
class="cmti-12">is a linear map near</span> \(a\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 875--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>WLOG, \(a = 0, f(a) = 0\). We label the &#xFB01;rst \(n\) coordinates of \(\RR ^m\) \(x_I\) and the last \(n-m\) \(x_{II}\). Then \(f'\) looks like \[\begin{pmatrix} \partial _If_I&amp;\partial _{II}f_I \end{pmatrix}\] and we
de&#xFB01;ne \(\psi :\RR ^m\to \RR ^m\) near \(0\) as \(f_I\) on the &#xFB01;rst \(n\) coordinates and \(x_{II}\) on the rest. Its derivative looks like \[\begin{pmatrix} \partial _If_I&amp;\partial _{II}f_I\\ 0&amp;I_{m-n} \end{pmatrix}\] so by
possibly reordering coordinates we can assume it is invertible and we can use Theorem <a 
href="#x1-9001r1">9.1<!--tex4ht:ref: invmapping --></a>
to locally make an inverse \(\phi :\RR ^m\to \RR ^m\) that &#xFB01;xes the coordinates \(x_{II}\). Now we have \[\begin{pmatrix}x_I \\ x_{II}\end{pmatrix} = (\psi \circ \phi )\begin{pmatrix}x_I\\x_{II}\end{pmatrix} = \begin{pmatrix} (f\circ \phi )(x)\\ x_{II}\end{pmatrix}\] <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
<!--l. 888--><p class="indent" >  <span 
class="cmti-12">so indeed</span> \(f\circ \phi \) <span 
class="cmti-12">is locally linear.</span>
</p>
  </div>
  <div class="newtheorem">
<!--l. 891--><p class="noindent" ><span class="head">
<a 
 id="x1-9004r4"></a>
<span 
class="cmbx-12">Theorem 9.4 </span>&#x0028;Implicit Function Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR ^m\to \RR ^n,m&gt;n\) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">near</span> \(0\) <span 
class="cmti-12">and</span> \(\det (\partial _{I}f(0))\neq 0\)<span 
class="cmti-12">, then for a small cell</span> \(I^m = I_I\times I_{II}\) <span 
class="cmti-12">near</span>
\(0\)<span 
class="cmti-12">, there is a</span> \(\cC ^1\) <span 
class="cmti-12">function</span> \(h:I_{II}\to I_{I}\) <span 
class="cmti-12">so that</span> \(f(x_I,x_{II}) = f(0)\) <span 
class="cmti-12">i&#xFB00;</span> \(x_I = h(x_{II})\)<span 
class="cmti-12">.</span>
</p>
                                                                                         
                                                                                         
  <div class="proof">
<!--l. 895--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By Theorem <a 
href="#x1-9003r3">9.3<!--tex4ht:ref: rankthmn --></a> we locally have a \(\cC ^1\) map \(\phi :\RR ^m\to \RR ^n\) that is a function \(g\) on the &#xFB01;rst \(n\) coordinates
and the identity on the last \(m-n\) coordinates such that \(f\circ \phi \) is the linear map \((x_I,x_{II})\mapsto (x_I)\). Then we de&#xFB01;ne \(h(x_{II}) = g(0,x_{II})\). Now
locally \(f(x) = 0\) i&#xFB00; \(x = \phi (y)\) and \((f\circ \phi )(y)=0\) i&#xFB00; \(x = \phi (0,y_{II})\) i&#xFB00; \(x_I = h(y_{II})=h(x_{II})\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 899--><p class="noindent" ><span class="head">
<a 
 id="x1-9005r5"></a>
<span 
class="cmbx-12">Corollary 9.5.</span>  </span> <span 
class="cmti-12">Locally a</span> \(\cC ^1\) \(k\)<span 
class="cmti-12">-submanifold of</span> \(\RR ^n\) <span 
class="cmti-12">is given by implicit</span> \(\cC ^1\) <span 
class="cmti-12">functions in</span> \(k\) <span 
class="cmti-12">variables.</span>
</p>
  <div class="proof">
<!--l. 903--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If \(V\) is such a submanifold, after a local change of coordinates at a point \(a\), it is a
linear subspace of \(\RR ^n\). We can collapse this subspace, which is \(\cC ^1\) and by Theorem <a 
href="#x1-9004r4">9.4<!--tex4ht:ref: impfnthm --></a> the kernel
composite of this with the local change of coordinates is given by implicit functions in \(k\)
variables. Conversely if \(V\) is given locally by implicit functions in \(k\) variables, those implicit
functions are a \(\cC ^1\) di&#xFB00;eomorphism to a linear embedding of \(\RR ^k\) in \(\RR ^n\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 907--><p class="indent" >  Note that the \(n=m=r\) case of the Rank Theorem below is the Inverse Mapping Theorem.
</p>
  <div class="newtheorem">
<!--l. 908--><p class="noindent" ><span class="head">
<a 
 id="x1-9006r6"></a>
<span 
class="cmbx-12">Theorem 9.6 </span>&#x0028;Rank Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR ^m \to \RR ^n\) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">near</span> \(a\) <span 
class="cmti-12">and</span> \(f'(x)\) <span 
class="cmti-12">is rank</span> \(r\) <span 
class="cmti-12">near</span> \(a\)<span 
class="cmti-12">, then there are</span> \(\cC ^1\) <span 
class="cmti-12">maps</span> \(\phi :\RR ^m\to \RR ^m\)
<span 
class="cmti-12">de&#xFB01;ned near</span> \(a\) <span 
class="cmti-12">and</span> \(\theta :\RR ^n\to \RR ^n\) <span 
class="cmti-12">de&#xFB01;ned near</span> \(f(a)\) <span 
class="cmti-12">such that</span> \(\theta \circ f\circ \phi \) <span 
class="cmti-12">is a linear map of rank</span> \(r\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 912--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>WLOG, \(a =0,f(a) =0\). By relabeling coordinates in the domain and range, we may assume that
the principle \(r\times r\) submatrix of \(f'(0)\) is invertible. Let us label the &#xFB01;rst \(r\) coordinates of \(\RR ^m\) \(x_I\) and the
last \(m-r\) \(x_{II}\). Then we can consider the map \(\psi :\RR ^m \to \RR ^m\) that is \(f\) on \(x_I\), and the identity on \(x_{II}\). By hypothesis, \(\psi '(0)\) is
invertible, so by the Inverse Mapping Theorem we can let \(\phi \) be its local inverse, which is \(g\) on \(x_I\)
and the identity on \(x_{II}\). Now we can call the &#xFB01;rst \(r\) coordinates of \(\RR ^n\) \(y_I\) and the last \(n-r\) \(y_{II}\). Now \(h = f \circ \phi \) is the
map that is \(x_I\) on the &#xFB01;rst \(r\) coordinates, and \(h_{II}\) on the last \(n-r\). \(h'\) looks like \[\begin{pmatrix} I_r&amp;0\\\partial _Ih_{II}&amp;\partial _{II}h_{II} \end{pmatrix}\] but since it is rank \(r\)
near \(a\) by the chain rule, we must have \(\partial _{II}h_{II} = 0\), so \(h\) only depends on \(x_I\). Now we can de&#xFB01;ne \(\theta :\RR ^n\to \RR ^n\) near \(0\) as
the identity on the &#xFB01;rst \(r\) coordinates, and \(y_{II} - h_{II}(y_I)\) on the last \(n-r\). Then \(\theta '(0)\) is rank \(n\), and \(\theta \circ h\) is \(x_I\) on the &#xFB01;rst \(r\)
coordinates and \(0\) on the last \(n-r\), which is linear. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 920--><p class="noindent" ><span class="head">
<a 
 id="x1-9007r7"></a>
<span 
class="cmbx-12">Corollary 9.7.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR ^m\to \RR ^n\) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">near a point</span> \(a\)<span 
class="cmti-12">, and</span> \(f'(x)\) <span 
class="cmti-12">is rank</span> \(r\) <span 
class="cmti-12">near</span> \(a\)<span 
class="cmti-12">, then the image of a small</span>
<span 
class="cmti-12">neighborhood around</span> \(a\) <span 
class="cmti-12">is a</span> \(\cC ^1\) \(r\)<span 
class="cmti-12">-submanifold of</span> \(\RR ^n\)<span 
class="cmti-12">, and the preimage of</span> \(f(a)\) <span 
class="cmti-12">is a</span> \(\cC ^1\) \(m-r\)<span 
class="cmti-12">-submanifold if</span> \(f\) <span 
class="cmti-12">is</span>
<span 
class="cmti-12">restricted close enough to</span> \(a\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 924--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Use Theorem <a 
href="#x1-9006r6">9.6<!--tex4ht:ref: rankthm --></a> to obtain \(\phi \) and \(\theta \). Now after applying these \(\cC ^1\) di&#xFB00;eomorphisms, the
preimage of \(f(a)\) is the kernel of a linear map, so is an open subset of \(\RR ^{m-r}\). Similarly, the image of a
neighborhood of \(a\) is an open subset of \(\RR ^r\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 928--><p class="noindent" ><span class="head">
<a 
 id="x1-9008r8"></a>
<span 
class="cmbx-12">Corollary 9.8 </span>&#x0028;Lagrange Multipliers&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(g:\RR ^n\to \RR \) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">in an open set</span> \(U\)<span 
class="cmti-12">, where</span> \(\cC ^1\) \(r\)<span 
class="cmti-12">-submanifold</span> \(V\) <span 
class="cmti-12">that is</span>
<span 
class="cmti-12">the locus of</span> \(f_i,1 \leq n-r\) <span 
class="cmti-12">in</span> \(U\)<span 
class="cmti-12">, any extremum</span> \(c\) <span 
class="cmti-12">of</span> \(g\) <span 
class="cmti-12">in</span> \(V\) <span 
class="cmti-12">must satisfy</span> \(\nabla g(c) = \sum _1^{n-r}\lambda _i\nabla f_i(c)\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 932--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>Note that for any parameterized curve \(\phi \) on \(V\) that sends \(0\) to \(c\), we have \(f_i(\phi (c))=0\), so by the chain
rule, \(\nabla f_i(c)\cdot \phi '(0) = 0\), so the tangent space is exactly the space perpendicular to the \(\nabla f_i\). Now in order to have
an extremum of \(g\) at \(c\) on \(V\), we need \(g'(c) = 0\), but then for any parameterized curve \(\phi \) on \(V\) sending \(0\) to \(x\),
again we have \(g(\phi (c)) = 0\), so by the chain rule, \(\nabla g(c)\) lies in the tangent space, so is a linear combination of
the \(\nabla f_i(c)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">10. </span> <a 
 id="x1-1000010"></a>Integration</h3>
  <div class="newtheorem">
<!--l. 938--><p class="noindent" ><span class="head">
<a 
 id="x1-10001r1"></a>
<span 
class="cmbx-12">Lemma 10.1.</span>  </span> <span 
class="cmti-12">A countable union</span> \(\cup _iU_i\) <span 
class="cmti-12">of measure</span> \(0\) <span 
class="cmti-12">sets is measure</span> \(0\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 942--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>For any \(\ee &gt;0\), cover each \(U_i\) with countably many cells summing to size \(\leq \frac{\ee }{2^{i+1}}\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 946--><p class="noindent" ><span class="head">
<a 
 id="x1-10002r2"></a>
<span 
class="cmbx-12">Theorem 10.2 </span>&#x0028;Riemann-Lebesgue Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">A bounded function</span> \(f\) <span 
class="cmti-12">on a closed cell</span> \(\Delta \) <span 
class="cmti-12">is Riemann</span>
<span 
class="cmti-12">integrable &#x0028;</span>\(\int _{*\Delta }f=\int ^{*\Delta }f\)<span 
class="cmti-12">&#x0029; i&#xFB00;</span> \(f\) <span 
class="cmti-12">is continuous almost everywhere.</span>
</p>
  <div class="proof">
<!--l. 949--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>Let \(M\) be the bound for \(f\). The set of points with oscillation \(\leq \ee \) is open, so the set of
discontinuities is compact, and since \(f\) is discontinuous on a set of measure \(0\), by compactness
this is actually content \(0\).
</p><!--l. 951--><p class="indent" >  If \(f\) is continuous almost everywhere, for any \(\ee &gt;0\), choose a partition \(\cP \) such that \(f\) varies by at
most \(\frac{\ee }{2|\Delta |}\) in each cell where \(f\) is continuous, and so that the the discontinuous points are covered
by cells with total content less than \(\frac{\ee }{2M|\Delta |}\). Then we have \(S^*(f,\cP )-S_*(f,\cP ) = \sum _{\Delta \in \cP }o_f(\Delta )|\Delta | &lt; \frac{\ee }{2} + \frac{\ee }{2} = \ee \) so \(\int _{*\Delta }f=\int ^{*\Delta }f\).
</p><!--l. 953--><p class="indent" >  Conversely if \(f\) is not continuous almost everywhere, by Lemma <a 
href="#x1-10001r1">10.1<!--tex4ht:ref: meas0 --></a> there is an \(\ee \) such that
the set of points with oscillation \(\geq \ee \) cannot be covered by cells of size smaller than \(\delta \), so \(\sum _{\Delta \in \cP }o_f(\Delta )|\Delta |\) is larger
than than \(\ee \delta \). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 957--><p class="noindent" >Note for some of the following theorems the conditions on the function may be made weaker, ie. it
can be just continuous, di&#xFB00;erentiable on the interior, with a bounded and almost everywhere
continuous derivative.
</p>
  <div class="newtheorem">
<!--l. 959--><p class="noindent" ><span class="head">
<a 
 id="x1-10003r3"></a>
<span 
class="cmbx-12">Theorem 10.3 </span>&#x0028;Second Fundamental Theorem of Calculus&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR \to \RR \) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">near</span> \([a,b]\)<span 
class="cmti-12">, then</span> \(\int _{[a,b]}f' = f(b)-f(a)\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 962--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>For any partition \(\cP \) we have by Proposition <a 
href="#x1-4002r2">4.2<!--tex4ht:ref: mvt --></a> that for each \([x,y] = \Delta \in \cP \), \(f'(c)(y-x) = f(y)-f(x)\) for a \(c \in \Delta \), and as this is
true for any partition, \(\int _*f'\leq f(b)-f(a) \leq \int ^*f'\), so by Theorem <a 
href="#x1-10002r2">10.2<!--tex4ht:ref: riemannintegrable --></a> we are done. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 966--><p class="noindent" ><span class="head">
<a 
 id="x1-10004r4"></a>
<span 
class="cmbx-12">Corollary 10.4 </span>&#x0028;Integration by Parts&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(f,g\) <span 
class="cmti-12">are</span> \(\cC ^1\) <span 
class="cmti-12">near an interval</span> \([a,b]\)<span 
class="cmti-12">, then</span> \(\int _{[a,b]}fg' = f(b)g(b)-f(a)g(a) - \int _{[a,b]}f'g\)
</p>
  <div class="proof">
<!--l. 970--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>This follows from linearity, Corollary <a 
href="#x1-4004r4">4.4<!--tex4ht:ref: prodquotrule --></a>, and Theorem <a 
href="#x1-10003r3">10.3<!--tex4ht:ref: 2ndfdthmcalc --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 974--><p class="noindent" ><span class="head">
<a 
 id="x1-10005r5"></a>
<span 
class="cmbx-12">Theorem 10.5 </span>&#x0028;First Fundamental Theorem of Calculus&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR \to \RR \) <span 
class="cmti-12">is continuous on the interval</span>
\([a,b]\)<span 
class="cmti-12">, then</span> \(F(x) = \int _a^xf\) <span 
class="cmti-12">is continuous on the interval and</span> \(F'(x) = f(x)\) <span 
class="cmti-12">in the interior.</span>
</p>
  <div class="proof">
<!--l. 978--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>\[F(x+h)-F(x)-f(x)h=\int _x^{x+h}f(t)-f(x)dt\] and as \(|f(t)-f(x)| \leq \ee \) for small enough \(h\) and any \(\ee &gt;0\), we have \[\int _x^{x+h}f(t)-f(x)dt \leq \int _x^{x+h}\ee \leq \ee h\] <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 984--><p class="noindent" ><span class="head">
<a 
 id="x1-10006r6"></a>
<span 
class="cmbx-12">Theorem 10.6 </span>&#x0028;Linearity of the Integral&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f,g\) <span 
class="cmti-12">are Riemann integrable and</span> \(D\) <span 
class="cmti-12">is a Jordan</span>
<span 
class="cmti-12">domain,</span> \(\int _D(c_1f+c_2g) = c_1\int _Df+c_2\int _Dg\)
</p>
  </div>
  <div class="proof">
<!--l. 988--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Scaling is obvious, so it su&#xFB03;ces to prove \(\int _D (f+g) = \int _D f + \int _D g\). To see this, choose partitions \(\cP _1\) and \(\cP _2\) so that
\(S_*(f,\cP _1),S_*(g,\cP _2)\) di&#xFB00;er by at most \(\frac{\ee }{2}\) from \(\int _D f,\int _D g\). Then taking a common re&#xFB01;nement \(\cP \), we have that \(S_*(f+g)\) di&#xFB00;ers from \(\int _D (f+g)\) by
at most \(\ee \). A similar argument can be made for the upper integral. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
                                                                                         
                                                                                         
  <div class="newtheorem">
<!--l. 991--><p class="noindent" ><span class="head">
<a 
 id="x1-10007r7"></a>
<span 
class="cmbx-12">Theorem 10.7 </span>&#x0028;Positivity of the Integral&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f\geq 0\) <span 
class="cmti-12">on</span> \(D\)<span 
class="cmti-12">, then</span> \(\int _Df\geq 0\)<span 
class="cmti-12">. If</span> \(f\geq g\) <span 
class="cmti-12">on</span> \(D\)<span 
class="cmti-12">, then</span> \(\int _Df\geq \int _Dg\)<span 
class="cmti-12">. Also for any</span> \(f\)<span 
class="cmti-12">,</span>
\(|\int _Df|\leq \int _D|f|\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 994--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>The &#xFB01;rst is obvious by looking at any partition. The second follows from the &#xFB01;rst and
Theorem <a 
href="#x1-10006r6">10.6<!--tex4ht:ref: intlinearity --></a>. The last follows from the second by noting \(|f|\geq f,-f\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 998--><p class="noindent" ><span class="head">
<a 
 id="x1-10008r8"></a>
<span 
class="cmbx-12">Theorem 10.8.</span>  </span><span 
class="cmti-12">For a vector valued function</span> \(f\)<span 
class="cmti-12">,</span> \(\Vert \int _Df\Vert _2\leq \int _D\Vert f\Vert _2\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1002--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Let \(u\) be a unit vector in the direction of \(\int _Df\). Then by the Cauchy-Schwarz inequality and
linearity, we have \[\bigg \Vert \int _Df\bigg \Vert _2=u\cdot \int _Df = \int _Du\cdot f \leq \int _D\Vert f\Vert _2\] <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1007--><p class="noindent" ><span class="head">
<a 
 id="x1-10009r9"></a>
<span 
class="cmbx-12">Theorem 10.9 </span>&#x0028;Invariance of the Integral&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f,g\) <span 
class="cmti-12">are integrable on a Jordan domain</span> \(D\) <span 
class="cmti-12">and</span>
<span 
class="cmti-12">di&#xFB00;er on a set of content</span> \(0\)<span 
class="cmti-12">, then</span> \(\int _Df = \int _Dg\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
                                                                                         
                                                                                         
<!--l. 1011--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>It su&#xFB03;ces to show a function nonzero on a set of content \(0\) has integral \(0\), but this is
true by de&#xFB01;nition of content \(0\), and the fact that the function must be bounded. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1015--><p class="noindent" ><span class="head">
<a 
 id="x1-10010r10"></a>
<span 
class="cmbx-12">Theorem 10.10 </span>&#x0028;Additivity of the Integral&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f\) <span 
class="cmti-12">is integrable on</span> \(D,E\)<span 
class="cmti-12">, Jordan domains whose</span>
<span 
class="cmti-12">intersection is content</span> \(0\)<span 
class="cmti-12">, then</span> \(\int _{D\cup E}f = \int _{D}f + \int _{E}f\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1018--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If \(\chi \) denotes the characteristic function, then \(\int _{D\cup E}f = \int _{D\cup E}(\chi _Df +\chi _Ef) = \int _Df + \int _Ef\), where we have ignored the boundary as
it is content \(0\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1022--><p class="noindent" ><span class="head">
<a 
 id="x1-10011r11"></a>
<span 
class="cmbx-12">Theorem 10.11 </span>&#x0028;Fubini&#x2019;s Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(f\) <span 
class="cmti-12">is integrable in the product cell</span> \(\Delta = \Delta _I\times \Delta _{II}\)<span 
class="cmti-12">, and the functions</span> \(\int _{*\Delta _I}f,\int ^*_{\Delta _I}f\) <span 
class="cmti-12">are</span>
<span 
class="cmti-12">integrable, then</span> \(\int _\Delta f = \int _{\Delta _{II}}\int _{*\Delta _I}f = \int _{\Delta _{II}}\int ^*_{\Delta _I}f\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1025--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Suppose we have a partition \(\cP \) that is the product of the partitions \(\cP _I,\cP _{II}\). Then we have
\[\sum _{\Delta \in \cP }\inf _{x \in \Delta }f(x)|\Delta |=\sum _{\Delta _{II} \in \cP _{II}}\sum _{\Delta _I \in \cP _I}\inf _{x\in \Delta _I\times \Delta _{II}}f(x)|\Delta _I| |\Delta _{II}|\]\[\leq \sum _{\Delta _{II} \in \cP _{II}}\inf _{x_{II}\in \Delta _{II}}\sum _{\Delta _I \in \cP _I}\inf _{x_{I} \in \Delta _{I}}f(x_I,x_{II})|\Delta _I||\Delta _{II}| \]\[\leq \sum _{\Delta _{II} \in \cP _{II}}\inf _{x_{II}\in \Delta _{II}}\int _{*\Delta _{I}}f(x_I,x_{II})|\Delta _{II}|\]\[\leq \sum _{\Delta _{II} \in \cP _{II}}\sum _{\Delta _I\in \cP _I}\sup _{x_{II}\in \Delta _{II}}\sup _{x_I\in \Delta _{I}}f(x_I,x_{II})|\Delta _I||\Delta _{II}| \leq \sum _{\Delta \in \cP }\sup _{x \in \Delta }f(x)|\Delta | \] showing that \(\int _{*\Delta } f \leq \int _{\Delta _{II}}\int _{*\Delta _I}f\leq \int ^*_{\Delta }f\) but as \(f\) is integrable these are equalities. The other equality comes from
dualizing the argument. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
                                                                                         
                                                                                         
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1036--><p class="noindent" ><span class="head">
<a 
 id="x1-10012r12"></a>
<span 
class="cmbx-12">Theorem 10.12.</span>  </span><span 
class="cmti-12">If</span> \(f_n\) <span 
class="cmti-12">is a uniformly convergent sequence of integrable functions in a cell</span> \(\Delta \)<span 
class="cmti-12">, the limit</span>
\(f\) <span 
class="cmti-12">is integrable and</span> \(\lim _{n\to \infty }\int _\Delta f_n=\int _\Delta f\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1039--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>To see \(f\) is integrable, note that the union of the discontinuities of the \(f_n\) is measure \(0\)
by Lemma <a 
href="#x1-10001r1">10.1<!--tex4ht:ref: meas0 --></a>, so by Theorem <a 
href="#x1-3016r16">3.16<!--tex4ht:ref: uniflimitthm --></a> \(f\) is continuous away from these points. \(|\int _{\Delta } f-\sum _1^nf_n|\leq \int _{\Delta }| f-\sum _1^nf_n| \to 0\) as \(n \to \infty \), so this
concludes the proof. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1044--><p class="noindent" ><span class="head">
<a 
 id="x1-10013r13"></a>
<span 
class="cmbx-12">Proposition 10.13.</span>  </span> <span 
class="cmti-12">Any open set</span> \(U \subset \RR ^m\) <span 
class="cmti-12">is the union of countably many open Jordan domains</span> \(D_i\) <span 
class="cmti-12">with</span> \(\bar D_i \subset D_{i+1}\)
<span 
class="cmti-12">and the</span> \(D_i\) <span 
class="cmti-12">composed of interiors of unions of cells of a partition.</span>
</p>
  <div class="proof">
<!--l. 1047--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Intersect \(U\) with a ball radius \(n\), partition \(\RR ^m\) into cells of size \(\frac{1}{2^n}\) and take the interior of the
union of the pieces whose boundary is completely contained inside \(U\) as \(D_n\). To make sure \(\bar{D_n}\subset D_{n+1}\), note
that the boundary of \(D_n\) is compact, so is eventually covered by another \(D_i\), so we can take a nice
enough subsequence. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
                                                                                         
                                                                                         
<!--l. 1051--><p class="noindent" ><span class="head">
<a 
 id="x1-10014r14"></a>
<span 
class="cmbx-12">Lemma 10.14.</span>  </span> <span 
class="cmti-12">If</span> \(k&lt;n\) <span 
class="cmti-12">and</span> \(V\) <span 
class="cmti-12">is a</span> \(\cC ^1\) \(k\)<span 
class="cmti-12">-submanifold of</span> \(\RR ^n\)<span 
class="cmti-12">, then</span> \(V\) <span 
class="cmti-12">is measure</span> \(0\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1055--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By Lemma <a 
href="#x1-10001r1">10.1<!--tex4ht:ref: meas0 --></a> it su&#xFB03;ces to do this locally, and by Corollary <a 
href="#x1-9005r5">9.5<!--tex4ht:ref: submanchar --></a> \(V\) is locally given by
implicit \(\cC ^1\) functions \(g_i\), and so locally the derivatives are uniformly continuous, and so for any
we can cover \(V\) in the plane of the variables that the \(g_i\) are a function of &#xFB01;nitely many by cells
of height arbitrarily small. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1059--><p class="noindent" ><span class="head">
<a 
 id="x1-10015r15"></a>
<span 
class="cmbx-12">Theorem 10.15 </span>&#x0028;Change of Variables&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(D,E\) <span 
class="cmti-12">are open sets of</span> \(\RR ^n\)<span 
class="cmti-12">, and</span> \(\phi :D\to E\) <span 
class="cmti-12">is a</span> \(\cC ^1\) <span 
class="cmti-12">di&#xFB00;eomorphism,</span>
<span 
class="cmti-12">then if</span> \(f\) <span 
class="cmti-12">has a &#xFB01;nite improper integral on</span> \(E\)<span 
class="cmti-12">, then</span> \((f\circ \phi )|\det \phi '|\) <span 
class="cmti-12">has a &#xFB01;nite improper integral on</span> \(D\)<span 
class="cmti-12">, moreover</span>
\(\int _D^\smile (f\circ \phi )|\det \phi '| = \int _E^\smile f\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1063--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We will make a series of reductions of this problem. If \(E_i\) is a sequence of Jordan domains
covering \(E\) of the sort in Proposition <a 
href="#x1-10013r13">10.13<!--tex4ht:ref: jorddom --></a> with \(\bar{E_i}\subset E_{i+1}\), then \(\phi ^{-1}(E_i)\) is a sequence of Jordan domains &#x0028;the
boundaries are submanifolds by Corollary <a 
href="#x1-9007r7">9.7<!--tex4ht:ref: ranksubmanifold --></a> hence are measure \(0\) by Lemma <a 
href="#x1-10014r14">10.14<!--tex4ht:ref: submanmeas0 --></a>&#x0029; with \(\phi ^{-1}(\bar{E}_i) = \overline{\phi ^{-1}(E_i)}\subset \phi ^{-1}(E_{i+1})\)
covering \(E\), so it su&#xFB03;ces to show this where \(f\) is a positive function in a cell \(\Delta \) in \(E\). By passing to
re&#xFB01;nements &#x0028;Theorem <a 
href="#x1-10010r10">10.10<!--tex4ht:ref: intadditivity --></a>&#x0029; and applying Theorem <a 
href="#x1-9002r2">9.2<!--tex4ht:ref: c1decomp --></a>, Lemma <a 
href="#x1-3014r14">3.14<!--tex4ht:ref: lebesguenumber --></a>, and the chain rule, it
su&#xFB03;ces to prove this when \(\phi \) changes only \(1\) variable on a su&#xFB03;ciently small cell \(\Delta \).
</p><!--l. 1065--><p class="indent" >  Finally to reduce to when \(f\) is a constant on the cell, suppose this has been proven, and for
any partition \(\cP \) of the cell \(\Delta \), de&#xFB01;ne \(f^*\) as \(f\) on the boundary of each \(\Delta ' \in \cP \) and \(\sup _{\Delta '}f\) on the inside. Then we
have: \[S^*(f,\cP ) = \int _\Delta f^* = \int _{\phi ^{-1}\Delta }(f^*\circ \phi )|\det \phi '| \geq \int _{\phi ^{-1}\Delta }(f\circ \phi )|\det \phi '| \] so \(\int ^*_{\Delta }f\geq \int _{\phi ^{-1}\Delta }(f\circ \phi )|\det \phi '|\). Dually we get \(\int _{*\Delta }f\leq \int _{\phi ^{-1}\Delta }(f\circ \phi )|\det \phi '|\).
</p><!--l. 1071--><p class="indent" >  Now  we  consider  the  case  when  \(f\)  is  constant  &#x0028;even  \(f=1\)  su&#xFB03;ces&#x0029;  and  will  reduce  to  the
\(1\)-dimensional case. We label the coordinates in \(\phi ^{-1}(\Delta )\) \(x_1\) and \(x_{II}\) and in \(\Delta \) \(y_1\) and \(y_{II}\) to distinguish the
coordinate \(\phi \) changes. \(\phi '\) looks like \[\begin{pmatrix} \partial _1\phi _1&amp;\partial _{II}\phi _1\\ 0&amp;I_{n-1} \end{pmatrix}\] so \(\det \phi ' = \partial _1\phi _1\). Now from the \(1\)-dimensional case and Theorem <a 
href="#x1-10011r11">10.11<!--tex4ht:ref: fubini --></a>,
we get \[\int _\Delta 1 = \int _{y_{II} \in \Delta _{II}}\int _{y_1\in \Delta _1}1 = \int _{x_{II} \in \Delta _{II}}\int _{x_1 \in \phi ^{-1}\Delta _1}|\partial _1\phi _1|\]\[=\int _{x_{II}\in \Delta _{II}}\int _{x_1\in \phi ^{-1}\Delta _1}|\phi '| = \int _{\phi ^{-1}\Delta }|\phi '|\]
                                                                                         
                                                                                         
</p><!--l. 1079--><p class="indent" >  Finally for the case of \(1\) dimension, if \(\phi :[a,b]\to [c,d]\) is our function, then \(\int _{[c,d]}f = F(d)-F(c)\) by Theorem <a 
href="#x1-10005r5">10.5<!--tex4ht:ref: 1stfdthmcalc --></a>, and since \((F\circ \phi )' = (f\circ \phi )\phi '\)
and WMA \(\phi '&gt;0\) as \(\phi \) is a \(\cC ^1\) di&#xFB00;eomorphism &#x0028;one treats the \(\phi '&lt;0\) case similarly&#x0029;, we have \(\int _{[a,b]}(f\circ \phi )|\phi '| = \int _{[a,b]}(F\circ \phi )'\) = \(F(d)-F(c)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">11. </span> <a 
 id="x1-1100011"></a>Line Integrals</h3>
  <div class="newtheorem">
<!--l. 1083--><p class="noindent" ><span class="head">
<a 
 id="x1-11001r1"></a>
<span 
class="cmbx-12">Lemma 11.1.</span>  </span> <span 
class="cmti-12">If</span> \(\psi :[a,b]\to \RR ^n\) <span 
class="cmti-12">is a recti&#xFB01;able curve and equivalent to</span> \(\phi :[c,d]\to \RR ^n\)<span 
class="cmti-12">, then</span> \(\phi \) <span 
class="cmti-12">is recti&#xFB01;able.</span>
</p>
  <div class="proof">
<!--l. 1086--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Let \(h:[a,b]\to [c,d]\) be the map of equivalence. Then for any partition \(\cP \) of \([a,b]\), \(h(\cP )\) is a partition of \([c,d]\) yielding
the same lengths. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1090--><p class="noindent" ><span class="head">
<a 
 id="x1-11002r2"></a>
<span 
class="cmbx-12">Lemma 11.2.</span>  </span> <span 
class="cmti-12">If</span> \(\psi :[a,b]\to \RR ^n\) <span 
class="cmti-12">is a recti&#xFB01;able curve, then</span> \(L(\psi ) = L(\psi |_{[a,c]})+L(\psi |_{[c,b]})\) <span 
class="cmti-12">for any</span> \(c\in [a,b]\)<span 
class="cmti-12">, and</span> \(L(\psi )\geq \Vert \psi (b)-\psi (a)\Vert _2\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1093--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>For the &#xFB01;rst one, just add in the point \(c\) to any partition of \([a,b]\). For the second, look at
the trivial &#x0028;initial&#x0029; partition. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
                                                                                         
                                                                                         
  </div>
  <div class="newtheorem">
<!--l. 1097--><p class="noindent" ><span class="head">
<a 
 id="x1-11003r3"></a>
<span 
class="cmbx-12">Lemma 11.3.</span>  </span> <span 
class="cmti-12">If</span> \(\psi :[0,1]\to \RR ^n\) <span 
class="cmti-12">is a recti&#xFB01;able curve, then</span> \(s:[0,1]\to [0,L(\psi )]\)<span 
class="cmti-12">,</span> \(s(t) = L(\psi |_{[0,t]})\) <span 
class="cmti-12">is continuously monotonically increasing, and is</span>
<span 
class="cmti-12">constant on a subinterval</span> \([a,b]\) <span 
class="cmti-12">i&#xFB00;</span> \(\psi \) <span 
class="cmti-12">is.</span>
</p>
  <div class="proof">
<!--l. 1100--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Since \(L\) is non-negative, by Lemma <a 
href="#x1-11002r2">11.2<!--tex4ht:ref: rectcurvineq --></a> \(s\) is monotonic. If \(\psi \) is constant, certainly \(s\) is by
the same Lemma, and conversely if \(\psi \) is not constant, then since there is a nonzero partition,
\(s\) cannot be constant.
</p><!--l. 1102--><p class="indent" >  For continuity, for any \(\ee &gt;0\) and \(t_0 \in [0,1]\) we have \(|s(t)-s(t_0)| = L(\psi |_{[t_0,t]}) = (L(\psi |_{[t_0,t]})-L(\psi |_{[t_0,t]},\cP ))+L(\psi |_{[t_0,t]},\cP )\) and now we choose a \(\cP \) to bound this. In particular,
we have our partition of \([0,1]\) be less than \(\frac{\ee }{2}\) away from \(L(\psi )\) and we assume that \(f\) varies at most \(\frac{\ee }{2}\) in
each subinterval, which is at most \(\delta \) in length. As long as \(t \in (t_0-\delta ,t_0+\delta )\), we have then \((L(\psi |_{[t_0,t]})-L(\psi |_{[t_0,t]},\cP ))+L(\psi |_{[t_0,t]},\cP ) \leq \ee \), giving continuity.
<span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1106--><p class="noindent" ><span class="head">
<a 
 id="x1-11004r4"></a>
<span 
class="cmbx-12">Proposition 11.4.</span>  </span> <span 
class="cmti-12">A</span> \(\cC ^1\) <span 
class="cmti-12">parameterized curve</span> \(\psi :[0,1]\to \RR ^n\) <span 
class="cmti-12">is recti&#xFB01;able, its arc length given by</span> \(L(\psi ) = \int _{[0,1]}\Vert \psi '\Vert _2\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1110--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>For any partition \(\cP \) we have \[L(\psi ,\cP ) = \sum _{[a,b]\in \cP }\Vert \psi (b)-\psi (a)\Vert _2= \sum _{[a,b]\in \cP }\bigg \Vert \int _{[a,b]}\psi '\bigg \Vert _2 \leq \sum _{[a,b]\in \cP }\int _{[a,b]}\Vert \psi '\Vert _2=\int _{[0,1]}\Vert \psi '\Vert _2\] so \(\psi \) is recti&#xFB01;able. For the second part, if \(h&gt;0\) is small, \(u\) is the
unit vector in the direction of \(\psi '(t)\) and \(\tau \) comes from Theorem <a 
href="#x1-4002r2">4.2<!--tex4ht:ref: mvt --></a> we have from Cauchy-Schwarz
inequality and Lemma <a 
href="#x1-11002r2">11.2<!--tex4ht:ref: rectcurvineq --></a>: \[|u\cdot \psi '(\tau )|=\frac{1}{h}|u\cdot (\psi (t+h)-\psi (t))| \leq \frac{1}{h}(s(t+h)-s(t)) \leq \frac{1}{h} \int _{[t,t+h]}\Vert \psi '\Vert _2\] Letting \(h\to 0\) on the left we get \(\Vert \phi '(t)\Vert _2\), and on the right we also get \(\Vert \phi '(t)\Vert _2\) by
Theorem <a 
href="#x1-10003r3">10.3<!--tex4ht:ref: 2ndfdthmcalc --></a>, so the middle, which is \(s'(t)\), must be that &#x0028;we also treat \(h&lt;0\) similarly&#x0029;. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
                                                                                         
                                                                                         
<!--l. 1119--><p class="noindent" >The arc length gives a natural parameterization of a curve for integration of a function, but this is
unnecessary for line integration over a vector &#xFB01;eld. A vector &#xFB01;eld should be thought of as a section
of the tangent bundle.
</p>
  <div class="newtheorem">
<!--l. 1121--><p class="noindent" ><span class="head">
<a 
 id="x1-11005r5"></a>
<span 
class="cmbx-12">Theorem 11.5.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR ^n\to \RR ^n\) <span 
class="cmti-12">is a vector &#xFB01;eld continuous near</span> \(\phi ([a,b])\) <span 
class="cmti-12">where</span> \(\phi \) <span 
class="cmti-12">is a</span> \(\cC ^1\) <span 
class="cmti-12">parameterized curve with</span>
\(\phi '\neq 0\)<span 
class="cmti-12">, then if</span> \(\tau \) <span 
class="cmti-12">is the unit tangent vector and</span> \(\psi \) <span 
class="cmti-12">is arc length parameterization,</span> \(\int _\psi (f\cdot \tau )ds = \int _{[a,b]}(f\circ \phi )\phi '\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1125--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If \(\psi \) denotes the parameterization by arc length, and \(\phi '\neq 0\), by Proposition <a 
href="#x1-11004r4">11.4<!--tex4ht:ref: C1paramcurve --></a> \(s\) is a \(\cC ^1\)
di&#xFB00;eomorphism with \(\psi \). Moreover, by the chain rule, \(\phi ' = (\psi \circ s^{-1})' = (\psi '\circ s^{-1})(s^{-1})'\) so we have by Theorem <a 
href="#x1-10015r15">10.15<!--tex4ht:ref: cov --></a> \(\int _{[a,b]} (f\circ \phi )\cdot \phi ' = \int _{[0,L(\phi )]}(f\circ \psi )\cdot \psi ' = \int _\psi (f\cdot \tau )ds\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1129--><p class="noindent" ><span class="head">
<a 
 id="x1-11006r6"></a>
<span 
class="cmbx-12">De&#xFB01;nition 11.6.</span>  </span><span 
class="cmti-12">A </span><span 
class="cmbxti-10x-x-120">conservative </span><span 
class="cmti-12">vector &#xFB01;eld</span> \(\RR ^n\to \RR ^n\) <span 
class="cmti-12">is one which is the gradient of a function</span> \(\RR ^n\to \RR \)<span 
class="cmti-12">,</span>
<span 
class="cmti-12">which is called its </span><span 
class="cmbxti-10x-x-120">potential</span><span 
class="cmti-12">.</span>
</p>
  </div>
  <div class="newtheorem">
<!--l. 1133--><p class="noindent" ><span class="head">
<a 
 id="x1-11007r7"></a>
<span 
class="cmbx-12">Theorem 11.7.</span>  </span> <span 
class="cmti-12">If</span> \(f:\RR ^n\to \RR ^n\) <span 
class="cmti-12">is a conservative vector &#xFB01;eld in a connected open set</span> \(U\) <span 
class="cmti-12">with</span> \(\cC ^1\) <span 
class="cmti-12">potential</span> \(h\)<span 
class="cmti-12">, and</span> \(\gamma \) <span 
class="cmti-12">is</span>
<span 
class="cmti-12">a</span> \(\cC ^1\) <span 
class="cmti-12">parameterized curve with unit tangent vector</span> \(\tau \) <span 
class="cmti-12">from</span> \(a\) <span 
class="cmti-12">to</span> \(b\) <span 
class="cmti-12">with</span> \(\gamma '\neq 0\)<span 
class="cmti-12">, then</span> \(\int _\gamma f\cdot \tau ds= h(b)-h(a)\)<span 
class="cmti-12">. That</span> \(f\) <span 
class="cmti-12">is</span> \(\cC ^1\) <span 
class="cmti-12">satis&#xFB01;es this</span>
<span 
class="cmti-12">property is also su&#xFB03;cient for it to be conservative.</span>
</p>
  <div class="proof">
<!--l. 1136--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>By Theorem <a 
href="#x1-11005r5">11.5<!--tex4ht:ref: lineint --></a> we may assume \(\gamma \) is arc length and by the chain rule, \[\frac{d}{ds}h = (h'\circ \gamma )\gamma ' = (\nabla h\circ \gamma )\cdot \gamma ' = (f\circ \gamma )\cdot \gamma ' = (f\circ \gamma )\cdot \tau \] so this follows
from Theorem <a 
href="#x1-10003r3">10.3<!--tex4ht:ref: 2ndfdthmcalc --></a>. For the converse, de&#xFB01;ne \(h(a) = \int _{\gamma _a}f\cdot \tau ds\) where \(\gamma _a\) is any path to \(a\) from a &#xFB01;xed point \(b\). Now
we can &#xFB01;nd the partial derivatives of \(h\) at a point \(a\) by integrating along a small path \(a\to a+\delta \) in one
component and taking the derivative. If \(u\) is the unit vector in the \(x_i\) direction which \(\delta \) is in, this
yields \(h(a+\delta )-h(a) = \int _{\gamma _{a,a+\delta }}f\cdot \tau ds = \int _{[0,\delta ]}(f(a+\delta )-f(a))\cdot u\) which by Theorem <a 
href="#x1-10003r3">10.3<!--tex4ht:ref: 2ndfdthmcalc --></a> shows \(f_i = \partial _i(h)\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <h3 class="sectionHead"><span class="titlemark">12. </span> <a 
 id="x1-1200012"></a>Exterior Algebras and Di&#xFB00;erential Forms</h3>
  <div class="newtheorem">
<!--l. 1143--><p class="noindent" ><span class="head">
<a 
 id="x1-12001r1"></a>
<span 
class="cmbx-12">De&#xFB01;nition 12.1.</span>  </span><span 
class="cmti-12">If</span> \(M\) <span 
class="cmti-12">is a &#xFB01;nite rank free</span> \(R\)<span 
class="cmti-12">-module with an ordered basis</span> \(m_1\dots m_n\)<span 
class="cmti-12">, an elememt</span> \(\omega \in \bigwedge ^r(M)\) <span 
class="cmti-12">is in</span>
<span 
class="cmti-12">the </span><span 
class="cmbxti-10x-x-120">reduced form </span><span 
class="cmti-12">if it is written as </span>\[\sum _{1\leq i_1&lt;\dots &lt; i_r\leq n} a_{i_1\dots i_r}m_{i_1}\wedge \dots \wedge m_{i_r}\]
</p>
  </div>
<!--l. 1147--><p class="noindent" >We will use <span 
class="cmbx-12">multi-index notation </span>writing \(\sum _Ia_Im_I\) for unreduced form, and \(\sum _I'a_Im_I\) for reduced form.
</p>
  <div class="newtheorem">
<!--l. 1149--><p class="noindent" ><span class="head">
<a 
 id="x1-12002r2"></a>
<span 
class="cmbx-12">De&#xFB01;nition 12.2.</span>  </span><span 
class="cmti-12">If</span> \(M\) <span 
class="cmti-12">is a &#xFB01;nite rank free</span> \(R\)<span 
class="cmti-12">-module with an ordered basis</span> \(x_1\dots x_n\)<span 
class="cmti-12">, the </span><span 
class="cmbxti-10x-x-120">Hodge duality</span>
<span 
class="cmbxti-10x-x-120">map </span><span 
class="cmti-12">denoted</span> \(*\) <span 
class="cmti-12">is de&#xFB01;ned as the linear map sending</span> \(x_I\in \bigwedge ^rM\) <span 
class="cmti-12">to</span> \(\ee (IJ)x_J\in \bigwedge ^{n-r}M\) <span 
class="cmti-12">where</span> \(\ee \) <span 
class="cmti-12">is the sign of the permutation,</span>
<span 
class="cmti-12">and</span> \(I,J\) <span 
class="cmti-12">are reduced.</span>
</p>
  </div>
  <div class="newtheorem">
<!--l. 1152--><p class="noindent" ><span class="head">
<a 
 id="x1-12003r3"></a>
<span 
class="cmbx-12">Lemma 12.3.</span>  </span><span 
class="cmti-12">The Hodge dual satis&#xFB01;es</span> \(*(*\omega ) = (-1)^{n(n-r)}\omega \)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1155--><p class="indent" >  <span class="head">
                                                                                         
                                                                                         
<span 
class="cmti-12">Proof.</span> </span>It su&#xFB03;ces to show this on \(dx_I\), that \(\ee (IJ)\ee (JI) = (-1)^{n(n-r)}\), but this is obvious by moving entries in \(J\) one at a
time across as transpositions. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1159--><p class="noindent" ><span class="head">
<a 
 id="x1-12004r4"></a>
<span 
class="cmbx-12">Lemma 12.4.</span>  </span><span 
class="cmti-12">If</span> \(\omega ,\sigma \in \bigwedge ^{r},\omega = \sum _I'f_Idx_I, \sigma = \sum _I'g_Idx_I\) <span 
class="cmti-12">then</span> \(\omega \wedge *\sigma = \sum _If_Ig_Idx_1\wedge \dots \wedge dx_n\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1162--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>All the terms in this wedge are \(0\) except the ones where the \(f_I\) and \(g_I\) correspond, in which
case the sign is \(\ee (IJ)\ee (IJ) = 1\) <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 1166--><p class="indent" >  We will work in the smooth category of \(\cC ^\infty \) functions for simplicity, but if one is careful it is
possible to treat \(\cC ^r\) functions. We think of the <span 
class="cmbx-12">de Rham complex</span> \(\bigwedge (D)\) of \(D\subset \RR ^n\) as the exterior algebra of the
\(\cC ^\infty (D)\)-module of smooth sections of the cotangent bundle \(T^*(D)\), where \(\bigwedge ^r(D)\) is a free module of rank \(\binom n r\). Elements of \(\bigwedge ^r(D)\)
are called <span 
class="cmbx-12">di&#xFB00;erential r-forms</span>. Since here \(D\) is in \(\RR ^n\), by choosing an orientation \(x_1\dots x_n\) of \(\RR ^n\), we
determine a basis for \(\bigwedge ^1(D)\), \(dx_1\dots dx_n\), namely \(dx_1\) is the section taking every point to the projection onto the \(x_i\)
coordinate &#x0028;recall that we have a canonical identi&#xFB01;cation of the tangent bundle with
\(\RR ^n\)&#x0029;.
</p><!--l. 1168--><p class="indent" >  The de Rham complex is a cochain complex because it has a di&#xFB00;erential \(d\), the exterior
derivative.
</p>
  <div class="newtheorem">
<!--l. 1170--><p class="noindent" ><span class="head">
<a 
 id="x1-12005r5"></a>
<span 
class="cmbx-12">De&#xFB01;nition 12.5.</span>  </span><span 
class="cmti-12">The </span><span 
class="cmbxti-10x-x-120">exterior derivative </span><span 
class="cmti-12">is de&#xFB01;ned by </span>\[d\bigg (\sum _If_Idx_I\bigg ) = \sum _Idf_Idx_I\] <span 
class="cmti-12">where</span> \(df\) <span 
class="cmti-12">is the dual of</span> \(f'\) <span 
class="cmti-12">written in</span>
<span 
class="cmti-12">the basis of the</span> \(dx\)<span 
class="cmti-12">s, ie</span> \(df = \sum _i\partial _idx_i\)
</p>
  </div>
                                                                                         
                                                                                         
<!--l. 1176--><p class="indent" >  Note that \(d(x_i) = dx_i\) where \(x_i\) is the \(i^{th}\) coordinate function &#x0028;indeed this is the meaning of \(dx_i\)&#x0029;. The cohomology of
the de Rham complex is called <span 
class="cmbx-12">de Rham cohomology</span>. The exterior derivative is characterized
by the following properties:
</p>
  <div class="newtheorem">
<!--l. 1178--><p class="noindent" ><span class="head">
<a 
 id="x1-12006r6"></a>
<span 
class="cmbx-12">Lemma 12.6.</span>  </span><span 
class="cmti-12">The exterior derivative</span> \(d\) <span 
class="cmti-12">is an</span> \(\RR \)<span 
class="cmti-12">-linear map satisfying</span> \(d(f) = df,(\omega \wedge \sigma ) = d\omega \wedge \sigma + (-1)^r \omega \wedge d\sigma \) <span 
class="cmti-12">and</span> \(dd\omega = 0\) <span 
class="cmti-12">where</span> \(\omega \in \bigwedge ^r(D), \sigma \in \bigwedge ^k(D)\)<span 
class="cmti-12">. Moreover it is</span>
<span 
class="cmti-12">characterized by these properties.</span>
</p>
  <div class="proof">
<!--l. 1181--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>\(d\) is clearly linear and satis&#xFB01;es the &#xFB01;rst property by de&#xFB01;nition. The second follows from
the product rule with the \((-1)^r\) coming from moving the derivative of each \(g\) in the \(\sigma \) to the right
place. For the third, it su&#xFB03;ces to show that \(ddf = 0\) for a function, but this is true by Theorem
<a 
href="#x1-4010r10">4.10<!--tex4ht:ref: partialssame --></a> since \(ddf = \sum _{i,j}\partial _i\partial _jdx_idx_j = 0\). To see that \(d\) is characterized by these properties, note that by induction \(d(dx_I) = 0\), so this
allows us to compute \(d(f\wedge dx_I)\) as de&#xFB01;ned. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 1185--><p class="indent" >  Now when we have a \(\cC ^\infty \) map \(\phi :D \to E\), we would like to be able to pullback di&#xFB00;erential forms to get a map
of cochain complexes, \(\phi ^*:\bigwedge (E)\to \bigwedge ^r(D)\). The pullback will serve as the chain map that turns de Rham cohomology
into a functor. To de&#xFB01;ne this, note that we have the pushforward, a map \(d\phi :TD\to TE\), and by taking its dual
&#x0028;transpose&#x0029; and exterior algebra, this gives a map \(\bigwedge (d\phi )^*:\bigwedge T^*E \to \bigwedge T^*D\). We can de&#xFB01;ne the <span 
class="cmbx-12">pullback </span>as the composite
in the diagram below:
</p>
<div class="center" 
>
<!--l. 1187--><p class="noindent" >
</p><!--l. 1190--><p class="noindent" >             <img 
src="Analysis0x.svg" alt="     &#x2227; &#x0028;T&#x2217;D &#x0029;       &#x2227; &#x0028;T &#x2217;E &#x0029;


&#x2227;
&#x03D5;&#x03D5;&#x03C9;&#x0028;&#x2217;d&#x0028;&#x03C9;&#x03D5;&#x0029;&#x2217;&#x0029;D              E "  />
</p></div>
<!--l. 1192--><p class="indent" >  How do we compute the pullback of \(\omega \)? Well &#xFB01;rst write \(\omega = \sum _If_Idx_I\). The \(f_I\) get sent by \(\phi \) to \(f\circ \phi \), and since \((d\phi )^*\) is \(\cC ^\infty (E)\)-linear
on sections, we only need to &#xFB01;nd out what happens to the \(dx_i\). To do this note that \(d\phi \) sends \((a,v)\mapsto (\phi (a),\phi '(a)v)\), which \(dx_i(\phi (a))\)
sends to \(\pi _{x_i}(\phi '(a))v = \phi _i'(a)v\) which is exactly what \(\sum _j\partial _j\phi _idy_j\) does, so \(dx_i \mapsto \sum _j\partial _j\phi _idy_j\), and we can compute the pullback as \[\phi ^*(fdy_1\wedge \dots \wedge dy_r) = \sum _J(f\circ \phi )\partial _{j_1}\phi _{i_1}\dots \partial _{j_r}\phi _{i_r}dx_{j_1}\wedge \dots \wedge dx_{j_r}\] which is
abbreviated as \[\phi ^*(f_I(y)dy_I) = f_I(\phi (y))\sum _J\partial _J\phi _Idx_J\]
                                                                                         
                                                                                         
</p>
  <div class="newtheorem">
<!--l. 1197--><p class="noindent" ><span class="head">
<a 
 id="x1-12007r7"></a>
<span 
class="cmbx-12">Lemma 12.7.</span>  </span><span 
class="cmti-12">Pullback is a contravariant functor and is a map of cochain complexes.</span>
</p>
  <div class="proof">
<!--l. 1201--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Let&#x2019;s consider the pullback of a map \(\phi :D\to E\), \(D \subset \RR ^m\), \(E\subset \RR ^n\). The pullback is the composite of maps along
the functor \(\bigwedge (d(-))^*\), which is a functor by the chain rule, so it is a functor. By de&#xFB01;nition pullback is
a map of algebras, and so we must check it commutes with the exterior derivative. For any
function \(f:E\to \RR \) we have by the chain rule \(\phi ^*(df) = \phi ^*(\sum _i\partial _ifdx_i) = \sum _i(\partial _if \circ \phi )(\sum _j\partial _j\phi _idy_j)\) \(= \sum _j\sum _i(\partial _jf\circ \phi )\partial _j\phi _idy_j = \sum _i\partial _i(f\circ \phi )dy_i= d(f\circ \phi ) = d(\phi ^*(f))\). Now by linearity we only need commutativity on \(\omega = fdx_I\) so \[\phi ^*(d\omega ) = \phi ^*(df\wedge dx_I) = \phi ^*(df)\wedge \phi ^*(dx_I) = d\phi ^*(f_I)\wedge \phi ^*(dx_I)\]\[ = d(\phi ^*(f_I)\phi ^*(dx_I)) = d(\phi ^*(\omega ))\]
<span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1206--><p class="noindent" ><span class="head">
<a 
 id="x1-12008r8"></a>
<span 
class="cmbx-12">Theorem 12.8 </span>&#x0028;Poincar&#x00E9; Lemma&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">The de Rham cohomology of a cell is</span> \(\RR \) <span 
class="cmti-12">in dimension</span> \(0\) <span 
class="cmti-12">and</span> \(0\)
<span 
class="cmti-12">elsewhere.</span>
</p>
  <div class="proof">
<!--l. 1209--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>We assume the cell is a unit cube. That the cohomology is \(\RR \) in dimension \(0\) follows from
the fact that the derivative is \(0\) i&#xFB00; the function is constant, and since the cell is connected,
there the function must globally be constant. For elsewhere, we consider a closed di&#xFB00;erential
\(r\)-form and show it is exact. To do this we induct on the largest \(k\) so that \(dx_k\) is used in the form.
For the base case, if \(\omega =fdx_1\) then \(dg = \omega \) where \(g = \int _0^{x_1}f\). Now to induct on \(k\), it su&#xFB03;ces by linearity and induction
to consider a \(r\)-form of the sort \(\omega = fdx_I\wedge dx_k\) where \(dx_I\) only use \(dx_1\dots dx_{k-1}\). As it is closed, we have \[0 = d\omega = \sum _{j=1}^n\partial _jfdx_j\wedge dx_I\wedge x_k \] so \(\partial _jf\) must be \(0\) for \(j&gt;k\),
so \(f\) is constant in those directions, so we de&#xFB01;ne \(h = \int _0^{x_{k}}f\) so that \(\partial _{k}h = f\), and setting \(\omega ' = hdx_I\) and changing \(\omega \) by \((-1)^{r-1}d\sigma \) we
are done by induction. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
                                                                                         
                                                                                         
  </div>
  <h3 class="sectionHead"><span class="titlemark">13. </span> <a 
 id="x1-1300013"></a>Integrals of Di&#xFB00;erential Forms</h3>
<!--l. 1218--><p class="noindent" >If we have an orientation \(y_1\dots y_n\) on \(\RR ^n\), we de&#xFB01;ne \(\int _Efdy_1\wedge \dots \wedge dy_n = \int _Ef\) for \(E\subset \RR ^n\).
</p>
  <div class="newtheorem">
<!--l. 1220--><p class="noindent" ><span class="head">
<a 
 id="x1-13001r1"></a>
<span 
class="cmbx-12">Theorem 13.1.</span>  </span> <span 
class="cmti-12">If</span> \(\phi :D\to E\) <span 
class="cmti-12">is a</span> \(\cC ^1\) <span 
class="cmti-12">di&#xFB00;eomorphism between two subsets of</span> \(\RR ^n\)<span 
class="cmti-12">, and</span> \(\omega \) <span 
class="cmti-12">is a continuous</span>
<span 
class="cmti-12">di&#xFB00;erential</span> \(n\)<span 
class="cmti-12">-form, then</span> \(\int _E\omega = \ee \int _D\phi ^*(\omega )\) <span 
class="cmti-12">where</span> \(\ee \) <span 
class="cmti-12">is the sign of the determinant of</span> \(\phi '\)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1224--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>If \(x_1\dots x_n\) is the orientation for \(D\), \(y_1\dots y_n\) is the orientation for \(E\), and \(\omega = fdy_I\) we have \(\int _D\phi ^*(\omega ) = \int _D(f\circ \phi )\det (\phi ')dx_I = \int _D(f\circ \phi )\det (\phi ') = \ee \int _Ef = \ee \int _E\omega \) by the change of
variables formula. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 1228--><p class="indent" >  A singular \(n\) cell in \(\RR ^m\) is a map from an oriented cell \(\Delta _n\) to \(\RR ^m\). We will mostly consider \(\cC ^\infty \) singular \(n\) cells.
Two \(\cC ^\infty \) singular \(n\) cells are equivalent if there is an orientation preserving \(\cC ^\infty \) di&#xFB00;eomorphism between
their domains that commutes with them. If we have a di&#xFB00;erential form \(\omega \) in an open set containing a
singular cell \(\phi :\Delta _n\to \RR ^m\), we de&#xFB01;ne \(\int _\phi \omega = \int _{\Delta _n}\phi ^*(\omega )\). By Theorem <a 
href="#x1-13001r1">13.1<!--tex4ht:ref: pullbackintegral --></a> this only depends on the equivalence class. Indeed given
a notion of integration, the pullback may be de&#xFB01;ned as the unique form satisfying \(\int _D\phi ^*(\omega ) = \int _{\phi (D)}\omega \) &#x0028;it is dual to the
pushforward of a chain&#x0029;.
</p><!--l. 1230--><p class="indent" >  Indeed if \(\phi \) is a singular \(1\) cell with \(\phi '\neq 0\), and \(\omega = \sum _1^nf_idx_i\), is a \(1\)-form, then if \(f\) is the vector &#xFB01;eld with components \(f_i\),
then \(\int _\phi \omega = \int _{[0,1]}(f\circ \phi )\cdot \phi ' = \int _\phi f\cdot \tau ds\).
</p><!--l. 1232--><p class="indent" >  Now since we have an inner product structure on \(\RR ^n\), we can identify covector with vectors, as a
covector corresponds to a unique vector such that dotting with that vector is that covector. Thus
covector &#xFB01;elds &#x0028;\(1\)-forms&#x0029; may be identi&#xFB01;ed with vector &#xFB01;elds. Similarly if \(\phi \) is a singular \(2\) cell with
orientation \(t_1,t_2\), and codomain \(\RR ^3\) with orientation \(x_1,x_2,x_3\), we can integrate a \(2\)-form associated with a vector
&#xFB01;eld \(f\) \(\omega = f_1dx_1+f_2dx_2+f_3dx_3\) over this cell, yielding \(\int _\phi \omega = \int _\Delta f(\phi (t))\cdot (\partial _1\phi \times \partial _2\phi )dt_1\wedge dt_2\) where \(\times \) denotes cross product. If \(\nu \) denotes the unit vector in the
direction of \(\partial _1\phi \times \partial _2\phi \), then \(\nu \) is the <span 
class="cmbx-12">normal unit vector</span>, and we can write \(\partial _1\phi \times \partial _2\phi = \nu \Vert \partial _1\phi \times \partial _2\phi \Vert _2\). Now we can interpret \(\int _\Delta \Vert \partial _1\phi \times \partial _2\phi \Vert _2 = \int _\phi 1dS\) as the
surface area, and write \(\int _\phi \omega = \int _\phi f\cdot \nu dS\).
</p><!--l. 1234--><p class="indent" >  We would like to integrate over chains rather than cells, so we de&#xFB01;ne a <span 
class="cmbx-12">singular</span> \(n\)<span 
class="cmbx-12">-chain </span>as a
&#xFB01;nite formal sum of singular \(n\) cells. We can let \(\Delta (x_1,\dots ,x_n)\) be the unit cell in \(\RR ^n\) with that orientation, and then
let its boundary be de&#xFB01;ned as the singular \(n-1\)-chain \[\partial \Delta (x_1,\dots ,x_n) = \sum _{i=1}^n\sum _{\ee = 0}^1(-1)^{i+\ee }\Delta (x_1,\dots ,\ee ,\dots ,x_n) \] where \(\ee \) denotes the part of the boundary
restricting \(x_i\) to \(\ee \). For a singular \(n\)-chain \(\phi \), we de&#xFB01;ne \(\partial \phi = \phi (\partial \Delta (x_1,\dots ,x_n))\). We then have our notions of boundary and cycle
for chains.
                                                                                         
                                                                                         
</p>
  <div class="newtheorem">
<!--l. 1239--><p class="noindent" ><span class="head">
<a 
 id="x1-13002r2"></a>
<span 
class="cmbx-12">Lemma 13.2.</span>  </span><span 
class="cmti-12">A boundary is a cycle.</span>
</p>
  <div class="proof">
<!--l. 1242--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>It su&#xFB03;ces to show this on the unit cell \(\Delta \). We have \[ \partial \partial \Delta (x_1,\dots ,x_n) = \sum _{i=1}^n\sum _{\ee = 0}^1(-1)^{i+\ee }\partial \Delta (x_1,\dots ,\ee ,\dots ,x_n) \]\[= \sum _{i=1}^n\sum _{\ee = 0}^1\sum _{i=1}^{n-1}\sum _{\ee '=0}^1(-1)^{i+\ee +j+\ee '}\Delta (x_1,\dots ,\ee ',\dots ,\ee ,\dots ,x_n) \] Now for a &#xFB01;xed \(\ee ',\ee \) and unordered
pair \((a,b)\), the term \(\Delta (x_1,\dots ,\ee ',\dots ,\ee ,\dots ,x_n)\) where \(\ee '\) and \(\ee \) take up the \(a\) and \(b\) slots respectively appear twice in this sum.
WLOG, \(a&lt;b\), and so we can have both \(i=a,j=b-1\) or \(i=b,j=a\). Then as \(i+j\) is a di&#xFB00;erent parity for these, the terms
cancel out, so this sum is \(0\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1250--><p class="noindent" ><span class="head">
<a 
 id="x1-13003r3"></a>
<span 
class="cmbx-12">Theorem 13.3 </span>&#x0028;Stoke&#x2019;s Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span> <span 
class="cmti-12">If</span> \(\cC \) <span 
class="cmti-12">is a</span> \(\cC ^\infty \) <span 
class="cmti-12">singular</span> \(n\)<span 
class="cmti-12">-chain and</span> \(\omega \) <span 
class="cmti-12">is a</span> \(n-1\)<span 
class="cmti-12">-form, then</span>
\(\int _\cC d\omega = \int _{\partial \cC } \omega \)<span 
class="cmti-12">.</span>
</p>
  <div class="proof">
<!--l. 1253--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>Since pullback commutes with exterior derivative and integration over chains is linear,
it su&#xFB03;ces to prove this on the unit cell \(\Delta (x_1,\dots ,x_n)\) for a \(n-1\)-form \(\omega = f dX_r\) where \(dX_r\) indicates \(dx_r\) is missing from \(dX = dx_1\wedge \dots \wedge dx_n\). This
way \(d\omega = (-1)^{r-1}\partial _rfdX\). Similarly let \(\Delta \) denote the cell, \(\Delta _{r,e}\) denote the boundary on the \(r^{th}\) side with \(\ee \), \(\Delta _r\) denote the cell
\(\Delta \) with the \(r\) dimension missing. Then from Theorem <a 
href="#x1-10011r11">10.11<!--tex4ht:ref: fubini --></a> and Theorem <a 
href="#x1-10003r3">10.3<!--tex4ht:ref: 2ndfdthmcalc --></a> we get \[ \int _\Delta d\omega = (-1)^{r-1}\int _\Delta \partial _rfdX= (-1)^{r-1}\int _{\Delta _r}\int _{\Delta (x_r)}\partial _rf\]\[= (-1)^{r+1}\int _{\Delta _{r,1}}f(x_1,\dots ,1,\dots ,x_n)+(-1)^{r}\int _{\Delta _{r,0}}f(x_1,\dots ,0,\dots ,x_n) = \int _{\partial \Delta }fdX_r \] Where
\(\int _{\partial \Delta }fdX_r\) vanishes on all other parts of the boundary as on \(\Delta _{i,\ee }\) with \(i\neq r\) the \(x_i\) part is constant, so pulling
back yields the form \(0\). <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
                                                                                         
                                                                                         
<!--l. 1262--><p class="indent" >  Indeed nothing deep is going on here, one may de&#xFB01;ne the exterior derivative as the map
so that Theorem <a 
href="#x1-13003r3">13.3<!--tex4ht:ref: stokesthm --></a> holds. Note that there is a pairing between chains and forms
&#x0028;homology and cohomology&#x0029;, and thus Theorem <a 
href="#x1-13003r3">13.3<!--tex4ht:ref: stokesthm --></a> says simply that \(d\) is adjoint to \(\partial \) for this
pairing.
</p>
  <div class="newtheorem">
<!--l. 1264--><p class="noindent" ><span class="head">
<a 
 id="x1-13004r4"></a>
<span 
class="cmbx-12">Corollary 13.4.</span>  </span><span 
class="cmti-12">If</span> \(H_1(U,\ZZ ) = 0\) <span 
class="cmti-12">for an open set</span> \(U \subset \RR ^n\)<span 
class="cmti-12">, then any closed</span> \(\cC ^\infty \) <span 
class="cmti-12">di&#xFB00;erential</span> \(1\)<span 
class="cmti-12">-form is exact.</span>
</p>
  <div class="proof">
<!--l. 1268--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>By Theorem <a 
href="#x1-11007r7">11.7<!--tex4ht:ref: conservative --></a> it su&#xFB03;ces to show line integrals depend only on the start and end,
but this follows from the hypothesis and Theorem <a 
href="#x1-13003r3">13.3<!--tex4ht:ref: stokesthm --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1272--><p class="noindent" ><span class="head">
<a 
 id="x1-13005r5"></a>
<span 
class="cmbx-12">Corollary 13.5.</span>  </span> <span 
class="cmti-12">If</span> \(D\) <span 
class="cmti-12">is the image of a simple &#x0028;orientation preserving di&#xFB00;eomorphism&#x0029; singular</span> \(\cC ^\infty \)
\(n\)<span 
class="cmti-12">-chain</span> \(\cC \) <span 
class="cmti-12">and</span> \(\omega = \sum _if_idX_i\) <span 
class="cmti-12">is an</span> \(n-1\)<span 
class="cmti-12">-form, then </span>\[\int _D\sum _i(-1)^{i-1}\partial _if_i = \int _{\partial \cC }\omega \]
</p>
  <div class="proof">
<!--l. 1275--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>This follows from Theorem <a 
href="#x1-13003r3">13.3<!--tex4ht:ref: stokesthm --></a> and Theorem <a 
href="#x1-13001r1">13.1<!--tex4ht:ref: pullbackintegral --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
  <div class="newtheorem">
<!--l. 1279--><p class="noindent" ><span class="head">
<a 
 id="x1-13006r6"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Corollary 13.6 </span>&#x0028;Green&#x2019;s Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(D\) <span 
class="cmti-12">is the image of a simple</span> \(\cC ^\infty \) <span 
class="cmti-12">singular</span> \(2\)<span 
class="cmti-12">-chain</span> \(\cC \)<span 
class="cmti-12">, then</span>
\[\int _{\partial \cC }f_1dx_1+f_2dx_2 = \int _D\partial _1f_2-\partial _2f_1\]
</p>
  <div class="proof">
<!--l. 1283--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>This is a special case of Theorem <a 
href="#x1-13005r5">13.5<!--tex4ht:ref: imagestokes --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 1287--><p class="indent" >  Green&#x2019;s theorem can be used to calculate area by integrating on the boundary the form \(x_1dx_2\)
or \(-x_2dx_1\) for example. For the case of \(\partial \cC \) being a curve \(\gamma \), we can get \(\int _\gamma f\cdot \nu ds = \int _D \partial _1f_1+\partial _2f_2\) where \(\nu \) is the outward
pointing normal vector de&#xFB01;ned as the unit vector in the direction of \(\begin{pmatrix} \partial _1\gamma _2\\ -\partial _1\gamma _1 \end{pmatrix}\). Then the left side is
interpreted as work done by a vector &#xFB01;eld pushing a particle, and the right is interpreted as
total &#xFB02;ow of a substance moving across the curve with velocity given by the vector
&#xFB01;eld.
</p>
  <div class="newtheorem">
<!--l. 1292--><p class="noindent" ><span class="head">
<a 
 id="x1-13007r7"></a>
<span 
class="cmbx-12">Corollary 13.7 </span>&#x0028;Gauss&#x2019;s Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(D\) <span 
class="cmti-12">is the image of a simple</span> \(\cC ^\infty \) <span 
class="cmti-12">singular</span> \(3\)<span 
class="cmti-12">-chain</span> \(\cC \)<span 
class="cmti-12">, then</span>
\[\int _{\partial \cC }f_1dx_2\wedge dx_3+f_2dx_3\wedge dx_1 + f_3dx_1\wedge dx_2 = \int _D\sum _{i=1}^3\partial _if_i\]
</p>
  <div class="proof">
<!--l. 1296--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>This is a special case of Theorem <a 
href="#x1-13005r5">13.5<!--tex4ht:ref: imagestokes --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 1300--><p class="indent" >  This can also be used to calculate volume, and can similarly be written as \(\int _\gamma f\cdot \nu dS = \int _D \nabla \cdot f\), which can be
interpreted as &#xFB02;ow and amount of substance created.
</p>
  <div class="newtheorem">
<!--l. 1302--><p class="noindent" ><span class="head">
<a 
 id="x1-13008r8"></a>
                                                                                         
                                                                                         
<span 
class="cmbx-12">Corollary 13.8 </span>&#x0028;Kelvin-Stokes Theorem&#x0029;<span 
class="cmbx-12">.</span>  </span><span 
class="cmti-12">If</span> \(\cC \) <span 
class="cmti-12">is a</span> \(\cC ^1\) <span 
class="cmti-12">singular</span> \(2\)<span 
class="cmti-12">-chain, then </span>\[ \int _{\partial \cC }f\cdot \tau ds = \int _{\cC }(\nabla \times f)\cdot \nu dS \]
</p>
  <div class="proof">
<!--l. 1308--><p class="indent" >  <span class="head">
<span 
class="cmti-12">Proof.</span> </span>This is a special case of Theorem <a 
href="#x1-13003r3">13.3<!--tex4ht:ref: stokesthm --></a>. <span class="qed"><span 
class="msam-10x-x-120">&#x25A1;</span></span>
</p>
  </div>
  </div>
<!--l. 1312--><p class="indent" >  As an interpretation of the curl, let \(f\) be a vector &#xFB01;eld in \(\RR ^3\), and we can integrate around a
tiny circle \(\gamma _\ee \) around a point \(a\) which bounds a disk \(D_\ee \) which has its unit normal vector \(\nu \)
pointing in the direction of \(\nabla \times f(a)\). Then by Kelvin-Stokes, \(\frac{1}{\pi \ee ^2}\int _{\gamma _\ee }f\cdot \tau ds = \frac{\int _{D_\ee }(\nabla \times f)\cdot \nu dS}{\int _{D_\ee }dS}\). But as the curl is continuous, we get
\[ \lim _{\ee \to 0}\frac{1}{\pi \ee ^2}\int _{\gamma _\ee }f\cdot \tau ds = \Vert \nabla \times f(a)\Vert _2 \]
</p>
   
</body> 
</html>
                                                                                         
                                                                                         
                                                                                         


